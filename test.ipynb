{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6169367a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcb8643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily columns: ['lake_id', 'date', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src', 'engine', 'area_m2', 'expected_pixels_geom', 'lake_name', 'empiric_n_valid_max', 'pct_valid_geom', 'pct_valid_emp', 'qc_low_cov_geom', 'qc_low_cov_emp', 'qc_tiny_abs_pix', 'qc_is_valid']\n",
      "  lake_id                date product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1 2024-01-01 13:52:31   daily      NaN     NaN      NaN   \n",
      "1    GL-1 2024-01-02 13:36:49   daily      NaN     NaN      NaN   \n",
      "2    GL-1 2024-01-03 14:02:06   daily      NaN     NaN      NaN   \n",
      "3    GL-1 2024-01-04 13:40:26   daily      NaN     NaN      NaN   \n",
      "4    GL-1 2024-01-05 13:48:41   daily      NaN     NaN      NaN   \n",
      "\n",
      "                                                 src   engine       area_m2  \\\n",
      "0  S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  2.488940e+10   \n",
      "1  S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  2.488940e+10   \n",
      "2  S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  2.488940e+10   \n",
      "3  S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  2.488940e+10   \n",
      "4  S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  2.488940e+10   \n",
      "\n",
      "   expected_pixels_geom lake_name  empiric_n_valid_max  pct_valid_geom  \\\n",
      "0                276549      Erie                  NaN            <NA>   \n",
      "1                276549      Erie                  NaN            <NA>   \n",
      "2                276549      Erie                  NaN            <NA>   \n",
      "3                276549      Erie                  NaN            <NA>   \n",
      "4                276549      Erie                  NaN            <NA>   \n",
      "\n",
      "   pct_valid_emp  qc_low_cov_geom  qc_low_cov_emp  qc_tiny_abs_pix  \\\n",
      "0            NaN             <NA>            <NA>                0   \n",
      "1            NaN             <NA>            <NA>                0   \n",
      "2            NaN             <NA>            <NA>                0   \n",
      "3            NaN             <NA>            <NA>                0   \n",
      "4            NaN             <NA>            <NA>                0   \n",
      "\n",
      "   qc_is_valid  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n",
      "CI_mean    1.0\n",
      "CI_p90     1.0\n",
      "dtype: float64\n",
      "         CI_mean  CI_p90\n",
      "lake_id                 \n",
      "GL-1           0       0\n",
      "GL-2           0       0\n",
      "GL-3           0       0\n",
      "GL-4           0       0\n",
      "GL-5           0       0\n",
      "Monthly columns: ['lake_id', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src', 'date', 'area_m2', 'expected_pixels_geom', 'lake_name', 'empiric_n_valid_max', 'pct_valid_geom', 'pct_valid_emp', 'qc_low_cov_geom', 'qc_low_cov_emp', 'qc_tiny_abs_pix', 'qc_is_valid']\n",
      "CI_mean    1.0\n",
      "CI_p90     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "QC_DIR = Path(\"datasets/processed/qc\")\n",
    "\n",
    "# 1) 看 daily 的列和基本情况\n",
    "dfd = pd.read_parquet(QC_DIR / \"greatlakes_daily_clean.parquet\")\n",
    "print(\"Daily columns:\", dfd.columns.tolist())\n",
    "print(dfd.head())\n",
    "\n",
    "# 2) 每个 lake 的 CI 有多少非 NaN\n",
    "print(dfd[[\"CI_mean\", \"CI_p90\"]].isna().mean())        # NaN 比例\n",
    "print(dfd.groupby(\"lake_id\")[[\"CI_mean\",\"CI_p90\"]].count().head())\n",
    "\n",
    "# 3) 看 monthly 的情况\n",
    "dfm = pd.read_parquet(QC_DIR / \"greatlakes_monthly_clean.parquet\")\n",
    "print(\"Monthly columns:\", dfm.columns.tolist())\n",
    "print(dfm[[\"CI_mean\", \"CI_p90\"]].isna().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be816abf",
   "metadata": {},
   "source": [
    "确认原始 lake parquet 里是不是已经全 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69562b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== lake_ci_daily.parquet ====\n",
      "['lake_id', 'date', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src', 'engine']\n",
      "  lake_id                      date product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1 2024-01-01 13:52:31+00:00   daily      NaN     NaN        0   \n",
      "1    GL-1 2024-01-02 13:36:49+00:00   daily      NaN     NaN        0   \n",
      "2    GL-1 2024-01-03 14:02:06+00:00   daily      NaN     NaN        0   \n",
      "3    GL-1 2024-01-04 13:40:26+00:00   daily      NaN     NaN        0   \n",
      "4    GL-1 2024-01-05 13:48:41+00:00   daily      NaN     NaN        0   \n",
      "\n",
      "                                                 src   engine  \n",
      "0  S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
      "1  S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
      "2  S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
      "3  S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
      "4  S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
      "CI_mean non-null 0 finite 0\n",
      "CI_p90 non-null 0 finite 0\n",
      "==== lake_ci_monthly.parquet ====\n",
      "['lake_id', 'time', 'product', 'CI_mean', 'CI_p90', 'n_valid', 'src']\n",
      "  lake_id                             time  product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1 2024-01-16 16:53:28.500000+00:00  monthly      NaN     NaN        0   \n",
      "1    GL-2 2024-01-16 16:53:28.500000+00:00  monthly      NaN     NaN        0   \n",
      "2    GL-3 2024-01-16 16:53:28.500000+00:00  monthly      NaN     NaN        0   \n",
      "3    GL-4 2024-01-16 16:53:28.500000+00:00  monthly      NaN     NaN        0   \n",
      "4    GL-5 2024-01-16 16:53:28.500000+00:00  monthly      NaN     NaN        0   \n",
      "\n",
      "                                                 src  \n",
      "0  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO...  \n",
      "1  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO...  \n",
      "2  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO...  \n",
      "3  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO...  \n",
      "4  S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CO...  \n",
      "CI_mean non-null 0 finite 0\n",
      "CI_p90 non-null 0 finite 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "P_LAKE_DAILY   = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\")\n",
    "P_LAKE_MONTHLY = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\")\n",
    "\n",
    "for p in [P_LAKE_DAILY, P_LAKE_MONTHLY]:\n",
    "    df = pd.read_parquet(p)\n",
    "    print(\"====\", p.name, \"====\")\n",
    "    print(df.columns.tolist())\n",
    "    print(df.head())\n",
    "    for c in [col for col in df.columns if \"CI\" in col or \"ci\" in col]:\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        print(c, \"non-null\", v.notna().sum(), \"finite\", (v.replace([float(\"inf\"), -float(\"inf\")], float(\"nan\")).notna()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3df88",
   "metadata": {},
   "source": [
    "Check original `.nc` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d588d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "\n",
      "=== DATASET SUMMARY ===\n",
      "<xarray.Dataset> Size: 27GB\n",
      "Dimensions:   (y: 15138, x: 26328, rgb: 3, eightbitcolor: 256)\n",
      "Coordinates:\n",
      "    lat       (y, x) float32 2GB ...\n",
      "    lon       (y, x) float32 2GB ...\n",
      "Dimensions without coordinates: y, x, rgb, eightbitcolor\n",
      "Data variables: (12/16)\n",
      "    rhos_400  (y, x) float32 2GB ...\n",
      "    rhos_412  (y, x) float32 2GB ...\n",
      "    rhos_443  (y, x) float32 2GB ...\n",
      "    rhos_490  (y, x) float32 2GB ...\n",
      "    rhos_510  (y, x) float32 2GB ...\n",
      "    rhos_560  (y, x) float32 2GB ...\n",
      "    ...        ...\n",
      "    rhos_709  (y, x) float32 2GB ...\n",
      "    rhos_754  (y, x) float32 2GB ...\n",
      "    rhos_865  (y, x) float32 2GB ...\n",
      "    rhos_884  (y, x) float32 2GB ...\n",
      "    CI_cyano  (y, x) float32 2GB ...\n",
      "    palette   (rgb, eightbitcolor) uint8 768B ...\n",
      "Attributes: (12/63)\n",
      "    product_name:                      S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CO...\n",
      "    project:                           Ocean Biology Processing Group (NASA/G...\n",
      "    source:                            satellite observations from OLCI-Senti...\n",
      "    temporal_range:                    7-hour\n",
      "    processing_version:                5\n",
      "    date_created:                      2025-03-26T04:05:22.000Z\n",
      "    ...                                ...\n",
      "    data_minimum:                      -0.37703314\n",
      "    data_maximum:                      1.6023113\n",
      "    history:                           l3mapgen par=S3M_OLCI_EFRNT.20240101.L...\n",
      "    title:                             Level-3 Mapped Data\n",
      "    instrument:                        OLCI\n",
      "    platform:                          Sentinel-3A,Sentinel-3B\n",
      "\n",
      "=== DATA VARS ===\n",
      "- rhos_400: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_412: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_443: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_490: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_510: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_560: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_620: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_665: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_674: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_681: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_709: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_754: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_865: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_884: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- CI_cyano: dims=('y', 'x'), attrs keys=['long_name', 'units', 'valid_min', 'valid_max', 'reference', 'display_scale', 'display_min', 'display_max']\n",
      "- palette: dims=('rgb', 'eightbitcolor'), attrs keys=[]\n",
      "\n",
      "=== COORDS ===\n",
      "- lat: dims=('y', 'x')\n",
      "- lon: dims=('y', 'x')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# 1) 选一个 daily 文件\n",
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "nc_path = sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\"))[0]\n",
    "print(\"Using file:\", nc_path)\n",
    "\n",
    "ds = xr.open_dataset(nc_path)\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(ds)\n",
    "\n",
    "print(\"\\n=== DATA VARS ===\")\n",
    "for name, da in ds.data_vars.items():\n",
    "    print(f\"- {name}: dims={da.dims}, attrs keys={list(da.attrs.keys())}\")\n",
    "\n",
    "print(\"\\n=== COORDS ===\")\n",
    "for name, da in ds.coords.items():\n",
    "    print(f\"- {name}: dims={da.dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdb1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CI overall stats ===\n",
      "shape: (15138, 26328)\n",
      "non-NaN ratio: 0.01559749112981797\n",
      "min: 4.9999991460936144e-05 max: 0.08100177347660065\n"
     ]
    }
   ],
   "source": [
    "var_name = \"CI_cyano\"  # 这里把名字换成你实际看到的那个\n",
    "da = ds[var_name]\n",
    "\n",
    "print(\"\\n=== CI overall stats ===\")\n",
    "print(\"shape:\", da.shape)\n",
    "arr = da.where(np.isfinite(da))  # 去掉 inf\n",
    "print(\"non-NaN ratio:\", float(arr.notnull().mean().values))\n",
    "print(\"min:\", float(arr.min().values), \"max:\", float(arr.max().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d5ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL-1 finite pixels: 0\n",
      "GL-2 finite pixels: 0\n",
      "GL-3 finite pixels: 0\n",
      "GL-4 finite pixels: 0\n",
      "GL-5 finite pixels: 0\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "P_LAKES = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "lakes = gpd.read_file(P_LAKES).to_crs(4326)\n",
    "\n",
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "nc_path = sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\"))[0]\n",
    "ds = xr.open_dataset(nc_path)\n",
    "\n",
    "# 把 CI 变量名换成你真正看到的那个\n",
    "var_name = \"CI_cyano\"\n",
    "da = ds[var_name]\n",
    "\n",
    "# 确保有 CRS 信息\n",
    "da = da.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "for _, row in lakes.iterrows():\n",
    "    geom = [row.geometry]\n",
    "    clipped = da.rio.clip(geom, lakes.crs, drop=False)\n",
    "    arr = clipped.values\n",
    "    finite = np.isfinite(arr)\n",
    "    print(row[\"lake_id\"], \"finite pixels:\", int(finite.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c13efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-150.3340606689453 -41.66587448120117\n"
     ]
    }
   ],
   "source": [
    "print(float(ds.lon.min().values), float(ds.lon.max().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75c63b",
   "metadata": {},
   "source": [
    "把 geometry 的经度从 −180–180 转到 0–360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69f6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.ops import transform as shp_transform\n",
    "\n",
    "def shift_lon_0_360(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    返回一个新的 GeoDataFrame 拷贝：\n",
    "    - 维持 EPSG:4326（纬度不变）\n",
    "    - 但经度从 [-180, 180] 映射到 [0, 360]\n",
    "    \"\"\"\n",
    "    def _shift_geom(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return geom\n",
    "        def _func(x, y, z=None):\n",
    "            x = np.array(x, dtype=\"float64\")\n",
    "            x = np.where(x < 0, x + 360.0, x)\n",
    "            if z is None:\n",
    "                return x, y\n",
    "            else:\n",
    "                return x, y, z\n",
    "        return shp_transform(_func, geom)\n",
    "\n",
    "    out = gdf.copy()\n",
    "    out[\"geometry\"] = out.geometry.apply(_shift_geom)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56a694",
   "metadata": {},
   "source": [
    "一个简化版的 extract_lakes_from_nc（核心逻辑）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d592a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "\n",
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "\n",
    "    # 1) CI 变量\n",
    "    da = ds[\"CI_cyano\"]\n",
    "\n",
    "    # 2) 写 CRS\n",
    "    da = da.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # 3) 准备 lakes 几何（注意经度范围）\n",
    "    lakes4326 = lakes_gdf.to_crs(4326)\n",
    "\n",
    "    # 判断 nc 的经度范围\n",
    "    lon = ds[\"lon\"]\n",
    "    lon_min = float(lon.min().values)\n",
    "    lon_max = float(lon.max().values)\n",
    "\n",
    "    if lon_min >= 0:  # 说明是 0–360\n",
    "        lakes_for_clip = shift_lon_0_360(lakes4326)\n",
    "    else:             # 已经是 -180–180\n",
    "        lakes_for_clip = lakes4326\n",
    "\n",
    "    # 4) 时间戳（简单写法：用属性/坐标，如果没有就从文件名解析）\n",
    "    if \"time\" in ds.coords:\n",
    "        # 如果 time 是长度 1 的坐标\n",
    "        t = pd.to_datetime(ds[\"time\"].values[0])\n",
    "    else:\n",
    "        # 例如文件名里有 20240101 这种，按你之前的规则 parse\n",
    "        # 这里给一个简单示例：\n",
    "        import re\n",
    "        m = re.search(r\"\\.(\\d{8})\\.\", nc_path)\n",
    "        if m:\n",
    "            t = pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "        else:\n",
    "            t = pd.NaT\n",
    "\n",
    "    rows = []\n",
    "    for _, row in lakes_for_clip.iterrows():\n",
    "        lake_id = row[lake_id_col]\n",
    "        geom = [row.geometry]\n",
    "\n",
    "        # rioxarray clip\n",
    "        sub = da.rio.clip(geom, lakes_for_clip.crs, drop=False)\n",
    "        arr = sub.values\n",
    "\n",
    "        finite = np.isfinite(arr)\n",
    "        n_valid = int(finite.sum())\n",
    "\n",
    "        if n_valid > 0:\n",
    "            vals = arr[finite]\n",
    "            ci_mean = float(vals.mean())\n",
    "            ci_p90  = float(np.percentile(vals, 90))\n",
    "        else:\n",
    "            ci_mean = np.nan\n",
    "            ci_p90  = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lake_id,\n",
    "            \"time\": t,\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": ci_mean,\n",
    "            \"CI_p90\": ci_p90,\n",
    "            \"n_valid\": n_valid,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e074",
   "metadata": {},
   "source": [
    "使用单日尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a81cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] read lakes: /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
      "  lake_id\n",
      "0    GL-1\n",
      "1    GL-2\n",
      "2    GL-3\n",
      "3    GL-4\n",
      "4    GL-5\n",
      "[INFO] open nc: /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "[INFO] lon range in nc: [-150.334, -41.666]\n",
      "[INFO] lon already in -180–180\n",
      "[INFO] parsed date: 2024-08-01 00:00:00\n",
      "\n",
      "=== RESULT DF ===\n",
      "  lake_id       date product  CI_mean  CI_p90  n_valid  \\\n",
      "0    GL-1 2024-08-01   daily      NaN     NaN        0   \n",
      "1    GL-2 2024-08-01   daily      NaN     NaN        0   \n",
      "2    GL-3 2024-08-01   daily      NaN     NaN        0   \n",
      "3    GL-4 2024-08-01   daily      NaN     NaN        0   \n",
      "4    GL-5 2024-08-01   daily      NaN     NaN        0   \n",
      "\n",
      "                                                 src  \n",
      "0  S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.a...  \n",
      "1  S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.a...  \n",
      "2  S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.a...  \n",
      "3  S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.a...  \n",
      "4  S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.a...  \n",
      "\n",
      "=== quick stats ===\n",
      "GL-1: n_valid=0, CI_mean=nan, CI_p90=nan\n",
      "GL-2: n_valid=0, CI_mean=nan, CI_p90=nan\n",
      "GL-3: n_valid=0, CI_mean=nan, CI_p90=nan\n",
      "GL-4: n_valid=0, CI_mean=nan, CI_p90=nan\n",
      "GL-5: n_valid=0, CI_mean=nan, CI_p90=nan\n",
      "\n",
      "[OK] saved debug parquet → /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/debug_lake_ci_20240801.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray  # 一定要 import 一下，才能用 .rio\n",
    "from shapely.ops import transform as shp_transform\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) 把经度从 [-180, 180] shift 到 [0, 360]\n",
    "# ---------------------------\n",
    "def shift_lon_0_360(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    返回一个新的 GeoDataFrame:\n",
    "    - 坐标系仍然是 EPSG:4326\n",
    "    - 但 geometry 中所有 x<0 的经度统一加 360\n",
    "    \"\"\"\n",
    "    def _shift_geom(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return geom\n",
    "\n",
    "        def _func(x, y, z=None):\n",
    "            x = np.array(x, dtype=\"float64\")\n",
    "            x = np.where(x < 0, x + 360.0, x)\n",
    "            if z is None:\n",
    "                return x, y\n",
    "            else:\n",
    "                return x, y, z\n",
    "\n",
    "        return shp_transform(_func, geom)\n",
    "\n",
    "    out = gdf.copy()\n",
    "    out[\"geometry\"] = out.geometry.apply(_shift_geom)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) 单日 lake 提取函数\n",
    "# ---------------------------\n",
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str = \"daily\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    从单个 L3m nc 文件中，对每个 lake 计算:\n",
    "      - CI_mean\n",
    "      - CI_p90\n",
    "      - n_valid (finite 像元数)\n",
    "\n",
    "    注意：\n",
    "    - 自动判断 lon 是否为 0–360，如是则对湖做 shift。\n",
    "    - 时间戳从文件名中解析 8 位日期（YYYYMMDD）。\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] open nc: {nc_path}\")\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "\n",
    "    # 1) 拿 CI 变量\n",
    "    if \"CI_cyano\" not in ds.data_vars:\n",
    "        raise KeyError(\"CI_cyano not found in dataset data_vars\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "\n",
    "    # 2) 写 CRS（经纬度）\n",
    "    da = da.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # 3) 准备 lakes 几何\n",
    "    lakes4326 = lakes_gdf.to_crs(4326)\n",
    "\n",
    "    # 判断 lon 范围\n",
    "    lon = ds[\"lon\"]\n",
    "    lon_min = float(lon.min().values)\n",
    "    lon_max = float(lon.max().values)\n",
    "    print(f\"[INFO] lon range in nc: [{lon_min:.3f}, {lon_max:.3f}]\")\n",
    "\n",
    "    if lon_min >= 0:\n",
    "        print(\"[INFO] lon is 0–360 → shift lakes to 0–360\")\n",
    "        lakes_for_clip = shift_lon_0_360(lakes4326)\n",
    "    else:\n",
    "        print(\"[INFO] lon already in -180–180\")\n",
    "        lakes_for_clip = lakes4326\n",
    "\n",
    "    # 4) 时间戳：从文件名 parse YYYYMMDD\n",
    "    m = re.search(r\"\\.(\\d{8})\\.\", Path(nc_path).name)\n",
    "    if m:\n",
    "        t = pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "    else:\n",
    "        t = pd.NaT\n",
    "    print(f\"[INFO] parsed date: {t}\")\n",
    "\n",
    "    # 5) 对每个 lake 做 clip\n",
    "    rows = []\n",
    "    for _, row in lakes_for_clip.iterrows():\n",
    "        lake_id = row[lake_id_col]\n",
    "        geom = [row.geometry]\n",
    "\n",
    "        sub = da.rio.clip(geom, lakes_for_clip.crs, drop=False)\n",
    "        arr = sub.values\n",
    "\n",
    "        finite = np.isfinite(arr)\n",
    "        n_valid = int(finite.sum())\n",
    "\n",
    "        if n_valid > 0:\n",
    "            vals = arr[finite]\n",
    "            ci_mean = float(vals.mean())\n",
    "            ci_p90 = float(np.percentile(vals, 90))\n",
    "        else:\n",
    "            ci_mean = np.nan\n",
    "            ci_p90 = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lake_id,\n",
    "            \"date\": t,\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": ci_mean,\n",
    "            \"CI_p90\": ci_p90,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"src\": Path(nc_path).name,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) main：用 2024-08-01 这一天测试\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 路径按你给的固定写死\n",
    "    P_LAKES = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "    NC_PATH = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")\n",
    "\n",
    "    print(\"[INFO] read lakes:\", P_LAKES)\n",
    "    lakes = gpd.read_file(P_LAKES)\n",
    "    print(lakes[[\"lake_id\"]].head())\n",
    "\n",
    "    df = extract_lakes_from_nc(str(NC_PATH), lakes, lake_id_col=\"lake_id\", product=\"daily\")\n",
    "\n",
    "    print(\"\\n=== RESULT DF ===\")\n",
    "    print(df)\n",
    "\n",
    "    # 顺便算一下简单统计\n",
    "    print(\"\\n=== quick stats ===\")\n",
    "    for _, r in df.iterrows():\n",
    "        print(\n",
    "            f\"{r['lake_id']}: n_valid={r['n_valid']}, \"\n",
    "            f\"CI_mean={r['CI_mean']}, CI_p90={r['CI_p90']}\"\n",
    "        )\n",
    "\n",
    "    # 可选：把结果存成一个小 parquet 方便后面对比\n",
    "    out_pq = NC_PATH.with_name(\"debug_lake_ci_20240801.parquet\")\n",
    "    df.to_parquet(out_pq, index=False)\n",
    "    print(f\"\\n[OK] saved debug parquet → {out_pq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79d2d6",
   "metadata": {},
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff11c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] read lakes: /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
      "  lake_id\n",
      "0    GL-1\n",
      "1    GL-2\n",
      "2    GL-3\n",
      "3    GL-4\n",
      "4    GL-5\n",
      "[INFO] open nc: /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "[INFO] lon range: -150.3340606689453 -41.66587448120117\n",
      "[INFO] lat range: 9.170879364013672 57.43088150024414\n",
      "\n",
      "[INFO] GL-1 bbox: (-83.46863511976093, 41.386089351877224, -78.86101258894625, 42.89921256238626)\n",
      "[RESULT] GL-1: n_valid=89675, CI_mean=0.00011738413013517857, CI_p90=5.0000002374872565e-05\n",
      "\n",
      "[INFO] GL-2 bbox: (-84.74594016316608, 43.008959363955704, -79.72171104277461, 46.54417264441878)\n",
      "[RESULT] GL-2: n_valid=392418, CI_mean=5.083527503302321e-05, CI_p90=5.0000002374872565e-05\n",
      "\n",
      "[INFO] GL-3 bbox: (-88.03564199558765, 41.62575045047001, -84.76135839606302, 46.09757222438803)\n",
      "[RESULT] GL-3: n_valid=2916, CI_mean=0.00010147361899726093, CI_p90=5.0000002374872565e-05\n",
      "\n",
      "[INFO] GL-4 bbox: (-79.87450768988488, 43.18588337418561, -75.77585380547868, 44.484774130454895)\n",
      "[RESULT] GL-4: n_valid=160890, CI_mean=6.132754788268358e-05, CI_p90=5.0000002374872565e-05\n",
      "\n",
      "[INFO] GL-5 bbox: (-92.19332866116895, 46.4165728419865, -84.36223274075998, 49.00969139722112)\n",
      "[RESULT] GL-5: n_valid=116474, CI_mean=5.0540027586976066e-05, CI_p90=5.0000002374872565e-05\n",
      "\n",
      "=== SUMMARY ===\n",
      "  lake_id  n_valid   CI_mean   CI_p90\n",
      "0    GL-1    89675  0.000117  0.00005\n",
      "1    GL-2   392418  0.000051  0.00005\n",
      "2    GL-3     2916  0.000101  0.00005\n",
      "3    GL-4   160890  0.000061  0.00005\n",
      "4    GL-5   116474  0.000051  0.00005\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# 路径\n",
    "P_LAKES = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "NC_PATH = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240801.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] read lakes:\", P_LAKES)\n",
    "    lakes = gpd.read_file(P_LAKES).to_crs(4326)\n",
    "    print(lakes[[\"lake_id\"]])\n",
    "\n",
    "    print(\"[INFO] open nc:\", NC_PATH)\n",
    "    ds = xr.open_dataset(NC_PATH)\n",
    "\n",
    "    lat = ds[\"lat\"]\n",
    "    lon = ds[\"lon\"]\n",
    "    ci  = ds[\"CI_cyano\"]\n",
    "\n",
    "    print(\"[INFO] lon range:\", float(lon.min().values), float(lon.max().values))\n",
    "    print(\"[INFO] lat range:\", float(lat.min().values), float(lat.max().values))\n",
    "\n",
    "    rows = []\n",
    "    for _, row in lakes.iterrows():\n",
    "        lake_id = row[\"lake_id\"]\n",
    "        minx, miny, maxx, maxy = row.geometry.bounds  # 经度, 纬度\n",
    "\n",
    "        print(f\"\\n[INFO] {lake_id} bbox:\", (minx, miny, maxx, maxy))\n",
    "\n",
    "        # 只用 bbox 做粗略掩膜\n",
    "        mask = (\n",
    "            (lon >= minx) & (lon <= maxx) &\n",
    "            (lat >= miny) & (lat <= maxy)\n",
    "        )\n",
    "\n",
    "        sub = ci.where(mask)\n",
    "        arr = sub.values  # 注意：这里会把 bbox 范围的数据读进内存\n",
    "\n",
    "        finite = np.isfinite(arr)\n",
    "        n_valid = int(finite.sum())\n",
    "\n",
    "        if n_valid > 0:\n",
    "            vals = arr[finite]\n",
    "            ci_mean = float(vals.mean())\n",
    "            ci_p90  = float(np.percentile(vals, 90))\n",
    "        else:\n",
    "            ci_mean = np.nan\n",
    "            ci_p90  = np.nan\n",
    "\n",
    "        print(f\"[RESULT] {lake_id}: n_valid={n_valid}, CI_mean={ci_mean}, CI_p90={ci_p90}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lake_id,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"CI_mean\": ci_mean,\n",
    "            \"CI_p90\": ci_p90,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9fca3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
