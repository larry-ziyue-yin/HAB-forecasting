{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73043a68",
   "metadata": {},
   "source": [
    "# Forecasting Freshwater Algal Bloom Levels Using Multisource Climate and Water-Quality Data\n",
    "\n",
    "*Course project of **STATS 402: Interdisciplinary Data Analysis**.*\n",
    "\n",
    "**Name:** Ziyue Yin\n",
    "\n",
    "**NetID:** zy166\n",
    "\n",
    "**How to use this Notebook *wisely*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac208a0",
   "metadata": {},
   "source": [
    "## Dataset: HydroLAKES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ef2d1",
   "metadata": {},
   "source": [
    "Lake polygons (including all attributes) in shapefile format can be found here: https://www.hydrosheds.org/products/hydrolakes. Simply scroll down to the \"Data download\" section and click on the link to start downloading the zipped file of:\n",
    "- Lake polygons (including all attributes) in shapefileformat (820MB).\n",
    "\n",
    "*TL;DR:* Click on https://data.hydrosheds.org/file/hydrolakes/HydroLAKES_polys_v10_shp.zip to download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de686a",
   "metadata": {},
   "source": [
    "This dataset will be later used as a standard area for filtering out the **Five Great Lakes** of the United States. **No** preprocessing is specified or needed thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9d58b",
   "metadata": {},
   "source": [
    "## Dataset: NASA OceanColor Inland Waters (ILW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eaa6",
   "metadata": {},
   "source": [
    "S3Merged-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/Merged-S3-ILW/.\n",
    "\n",
    "S3B-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/S3B-ILW/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419fb8",
   "metadata": {},
   "source": [
    "After downloading the datasets, the structure should be shown as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    " ├── ILW/\n",
    " │    ├── S3B/2024/CONUS_MO/\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240201_20240229.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    ├── Merged/2024/CONUS_DAY/\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70cade",
   "metadata": {},
   "source": [
    "### Data Structure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84bc5",
   "metadata": {},
   "source": [
    "First of all, let's glance at the **monthly** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3545ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b94fb",
   "metadata": {},
   "source": [
    "And also, the **daily** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a063b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3268c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
      "\n",
      "=== DATASET SUMMARY ===\n",
      "<xarray.Dataset> Size: 27GB\n",
      "Dimensions:   (y: 15138, x: 26328, rgb: 3, eightbitcolor: 256)\n",
      "Coordinates:\n",
      "    lat       (y, x) float32 2GB ...\n",
      "    lon       (y, x) float32 2GB ...\n",
      "Dimensions without coordinates: y, x, rgb, eightbitcolor\n",
      "Data variables: (12/16)\n",
      "    rhos_400  (y, x) float32 2GB ...\n",
      "    rhos_412  (y, x) float32 2GB ...\n",
      "    rhos_443  (y, x) float32 2GB ...\n",
      "    rhos_490  (y, x) float32 2GB ...\n",
      "    rhos_510  (y, x) float32 2GB ...\n",
      "    rhos_560  (y, x) float32 2GB ...\n",
      "    ...        ...\n",
      "    rhos_709  (y, x) float32 2GB ...\n",
      "    rhos_754  (y, x) float32 2GB ...\n",
      "    rhos_865  (y, x) float32 2GB ...\n",
      "    rhos_884  (y, x) float32 2GB ...\n",
      "    CI_cyano  (y, x) float32 2GB ...\n",
      "    palette   (rgb, eightbitcolor) uint8 768B ...\n",
      "Attributes: (12/63)\n",
      "    product_name:                      S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CO...\n",
      "    project:                           Ocean Biology Processing Group (NASA/G...\n",
      "    source:                            satellite observations from OLCI-Senti...\n",
      "    temporal_range:                    7-hour\n",
      "    processing_version:                5\n",
      "    date_created:                      2025-03-26T04:05:22.000Z\n",
      "    ...                                ...\n",
      "    data_minimum:                      -0.37703314\n",
      "    data_maximum:                      1.6023113\n",
      "    history:                           l3mapgen par=S3M_OLCI_EFRNT.20240101.L...\n",
      "    title:                             Level-3 Mapped Data\n",
      "    instrument:                        OLCI\n",
      "    platform:                          Sentinel-3A,Sentinel-3B\n",
      "\n",
      "=== DATA VARS ===\n",
      "- rhos_400: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_412: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_443: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_490: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_510: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_560: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_620: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_665: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_674: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_681: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_709: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_754: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_865: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- rhos_884: dims=('y', 'x'), attrs keys=['long_name', 'valid_min', 'valid_max', 'display_scale', 'display_min', 'display_max']\n",
      "- CI_cyano: dims=('y', 'x'), attrs keys=['long_name', 'units', 'valid_min', 'valid_max', 'reference', 'display_scale', 'display_min', 'display_max']\n",
      "- palette: dims=('rgb', 'eightbitcolor'), attrs keys=[]\n",
      "\n",
      "=== COORDS ===\n",
      "- lat: dims=('y', 'x')\n",
      "- lon: dims=('y', 'x')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "nc_path = sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\"))[0]\n",
    "print(\"Using file:\", nc_path)\n",
    "\n",
    "ds = xr.open_dataset(nc_path)\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(ds)\n",
    "\n",
    "print(\"\\n=== DATA VARS ===\")\n",
    "for name, da in ds.data_vars.items():\n",
    "    print(f\"- {name}: dims={da.dims}, attrs keys={list(da.attrs.keys())}\")\n",
    "\n",
    "print(\"\\n=== COORDS ===\")\n",
    "for name, da in ds.coords.items():\n",
    "    print(f\"- {name}: dims={da.dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b349",
   "metadata": {},
   "source": [
    "### Target Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbba54",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d9caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray  # 目前不再在裁剪中使用，但保留以防其他地方需要\n",
    "\n",
    "def infer_time_label(nc_path, ds, product=\"monthly\"):\n",
    "    \"\"\"\n",
    "    Return a pandas.Timestamp, try to infer from ds or filename.\n",
    "    product: 'monthly' or 'daily'\n",
    "    \"\"\"\n",
    "    # 1) Directly have time coordinate/variable\n",
    "    if \"time\" in ds.coords or \"time\" in ds.variables:\n",
    "        try:\n",
    "            tt = pd.to_datetime(ds[\"time\"].values)\n",
    "            tt = np.array(tt).reshape(-1)[0]\n",
    "            return pd.to_datetime(tt)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) Global attributes (common in L3M data)\n",
    "    start = ds.attrs.get(\"time_coverage_start\") or ds.attrs.get(\"start_time\")\n",
    "    end   = ds.attrs.get(\"time_coverage_end\")   or ds.attrs.get(\"end_time\")\n",
    "    if start and end:\n",
    "        try:\n",
    "            ts = pd.to_datetime(start)\n",
    "            te = pd.to_datetime(end)\n",
    "            if product == \"monthly\":\n",
    "                return ts + (te - ts) / 2\n",
    "            else:\n",
    "                return ts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Analyze the filename\n",
    "    fn = nc_path.split(\"/\")[-1]\n",
    "    if product == \"monthly\":\n",
    "        # ...YYYYMMDD_YYYYMMDD.L3m.MO...\n",
    "        m = re.search(r\"\\.(\\d{8})_(\\d{8})\\.L3m\\.MO\\.\", fn)\n",
    "        if m:\n",
    "            b, e = m.group(1), m.group(2)\n",
    "            ts = pd.to_datetime(b, format=\"%Y%m%d\")\n",
    "            te = pd.to_datetime(e, format=\"%Y%m%d\")\n",
    "            return ts + (te - ts) / 2\n",
    "    else:\n",
    "        # ...YYYYMMDD.L3m.DAY...\n",
    "        m = re.search(r\"\\.(\\d{8})\\.L3m\\.DAY\\.\", fn)\n",
    "        if m:\n",
    "            return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "    raise ValueError(\"Cannot infer time from dataset or filename: \" + fn)\n",
    "\n",
    "def clean_ci(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Filter out values out of physical range and remove near-zero values.\n",
    "    \"\"\"\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin):\n",
    "        da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax):\n",
    "        da = da.where(da <= vmax)\n",
    "\n",
    "    # drop near-zero (background) CI\n",
    "    thr = max(vmin, 5e-5) if np.isfinite(vmin) else 5e-5\n",
    "    da = da.where(da > thr)\n",
    "\n",
    "    return da\n",
    "\n",
    "def _extract_lakes_core_from_ds(\n",
    "    ds: xr.Dataset,\n",
    "    nc_path: str,\n",
    "    lakes_gdf: gpd.GeoDataFrame,\n",
    "    lake_id_col: str,\n",
    "    product: str,\n",
    "    time_label: pd.Timestamp,\n",
    "    engine: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    核心提取逻辑：针对一个 xarray.Dataset，用 lat/lon + bbox 做掩膜裁剪 CI_cyano。\n",
    "    兼容曲线网格(lat(y,x), lon(y,x))。\n",
    "    返回列：\n",
    "      lake_id, time, product, CI_mean, CI_p90, n_valid, src, engine\n",
    "    \"\"\"\n",
    "    if \"CI_cyano\" not in ds.data_vars:\n",
    "        raise KeyError(\"CI_cyano not found in dataset data_vars\")\n",
    "\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    # 期望 lat, lon 是 2D (y,x)，与 CI_cyano 维度一致\n",
    "    if \"lat\" not in ds.variables or \"lon\" not in ds.variables:\n",
    "        raise KeyError(\"lat / lon not found in dataset\")\n",
    "\n",
    "    lat = ds[\"lat\"]\n",
    "    lon = ds[\"lon\"]\n",
    "\n",
    "    if lat.shape != da.shape or lon.shape != da.shape:\n",
    "        # 极端情况下可以做广播，但对 ILW_CONUS 来说应该是一致的\n",
    "        raise ValueError(\n",
    "            f\"lat/lon shape {lat.shape}/{lon.shape} not matching CI_cyano {da.shape}\"\n",
    "        )\n",
    "\n",
    "    rows = []\n",
    "    for _, r in lakes_gdf.iterrows():\n",
    "        lid = r[lake_id_col]\n",
    "        geom = r.geometry\n",
    "        minx, miny, maxx, maxy = geom.bounds  # 经度, 纬度\n",
    "\n",
    "        # 使用 bbox 对 lat/lon 作初筛\n",
    "        mask = (\n",
    "            (lon >= minx) & (lon <= maxx) &\n",
    "            (lat >= miny) & (lat <= maxy)\n",
    "        )\n",
    "\n",
    "        sub = da.where(mask)\n",
    "        arr = np.asarray(sub.values)\n",
    "        finite = np.isfinite(arr)\n",
    "        n_valid = int(finite.sum())\n",
    "\n",
    "        if n_valid > 0:\n",
    "            vals = arr[finite].ravel()\n",
    "            mean_val = float(np.nanmean(vals))\n",
    "            p90      = float(np.nanquantile(vals, 0.9))\n",
    "        else:\n",
    "            mean_val, p90 = np.nan, np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lid,\n",
    "            \"time\":   pd.to_datetime(time_label),\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": mean_val,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"src\":     Path(nc_path).name,\n",
    "            \"engine\":  engine,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    nc_path: A single NetCDF file (S3B monthly or S3M daily)\n",
    "    lakes_gdf: A GeoDataFrame containing `lake_id` and `geometry` (EPSG:4326)\n",
    "    product: 'monthly' | 'daily'\n",
    "    Returns: One row per lake (timestamp of the file)\n",
    "    \"\"\"\n",
    "    nc_path = str(nc_path)\n",
    "    with xr.open_dataset(nc_path, engine=\"netcdf4\") as ds:\n",
    "        t = infer_time_label(nc_path, ds, product=product)\n",
    "        df = _extract_lakes_core_from_ds(\n",
    "            ds=ds,\n",
    "            nc_path=nc_path,\n",
    "            lakes_gdf=lakes_gdf,\n",
    "            lake_id_col=lake_id_col,\n",
    "            product=product,\n",
    "            time_label=t,\n",
    "            engine=\"netcdf4\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def set_spatial_dims_safe(da: xr.DataArray, ds: xr.Dataset | None = None) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    [遗留函数] 现在不再用于 lake 提取，但保留以兼容其他代码。\n",
    "    Make `da` geospatially aware for rioxarray:\n",
    "    - Prefer real dimension names present on `da` (lon/lat or x/y or variants);\n",
    "    - Only pass EXISTING dimension names to `rio.set_spatial_dims`;\n",
    "    - If dataset carries 1D lon/lat arrays matching x/y lengths, bind them as coords;\n",
    "    - Finally write CRS=EPSG:4326.\n",
    "\n",
    "    NOTE: If lon/lat are 2D (curvilinear), we DO NOT pass them as dims; we keep x/y dims.\n",
    "    \"\"\"\n",
    "    dims = list(da.dims)\n",
    "\n",
    "    # normalize common variants\n",
    "    def _has(*cands):\n",
    "        return any(c in dims for c in cands)\n",
    "\n",
    "    # Case A: dims already lon/lat\n",
    "    if \"lon\" in dims and \"lat\" in dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "    elif \"longitude\" in dims and \"latitude\" in dims:\n",
    "        out = da.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "\n",
    "    # Case B: dims are x/y (any case)\n",
    "    elif (_has(\"x\") and _has(\"y\")) or (_has(\"X\") and _has(\"Y\")):\n",
    "        # rename upper-case to lower-case to please rioxarray\n",
    "        rename_map = {}\n",
    "        if \"X\" in dims: rename_map[\"X\"] = \"x\"\n",
    "        if \"Y\" in dims: rename_map[\"Y\"] = \"y\"\n",
    "        out = da.rename(rename_map) if rename_map else da\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "        # try binding 1D lon/lat coords from ds if lengths match\n",
    "        if ds is not None:\n",
    "            try:\n",
    "                lon1d = None\n",
    "                lat1d = None\n",
    "                for lon_name in (\"lon\",\"longitude\"):\n",
    "                    if lon_name in ds.variables and ds[lon_name].ndim == 1 and ds[lon_name].sizes[ds[lon_name].dims[0]] == out.sizes[\"x\"]:\n",
    "                        lon1d = np.asarray(ds[lon_name].values)\n",
    "                        break\n",
    "                for lat_name in (\"lat\",\"latitude\"):\n",
    "                    if lat_name in ds.variables and ds[lat_name].ndim == 1 and ds[lat_name].sizes[ds[lat_name].dims[0]] == out.sizes[\"y\"]:\n",
    "                        lat1d = np.asarray(ds[lat_name].values)\n",
    "                        break\n",
    "                if lon1d is not None and lat1d is not None:\n",
    "                    out = out.assign_coords(x=lon1d, y=lat1d)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Case C: unknown names → rename the last two dimensions to y/x\n",
    "    elif len(dims) >= 2:\n",
    "        ydim, xdim = dims[-2], dims[-1]\n",
    "        out = da.rename({xdim: \"x\", ydim: \"y\"}).rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine spatial dims for {da.name!r}; dims={dims}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def looks_like_hdf5(path: Path) -> bool:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            sig = f.read(8)\n",
    "        return sig.startswith(b\"\\x89HDF\") or sig.startswith(b\"CDF\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _extract_one_with_h5netcdf(nc_path: Path,\n",
    "                               lakes_gdf: gpd.GeoDataFrame,\n",
    "                               lake_id_col: str,\n",
    "                               product: str = \"daily\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    兜底方案：用 h5netcdf 打开并在本函数内完成裁剪与统计。\n",
    "    现在同样使用 lat/lon + bbox 掩膜，不再调用 rioxarray.clip。\n",
    "    返回列同 extract_lakes_from_nc：\n",
    "      lake_id, time, product, CI_mean, CI_p90, n_valid, src, engine\n",
    "    \"\"\"\n",
    "    nc_path = Path(nc_path)\n",
    "    with xr.open_dataset(nc_path, engine=\"h5netcdf\", phony_dims=\"access\") as ds:\n",
    "        t = infer_time_label(str(nc_path), ds, product=product)\n",
    "        df = _extract_lakes_core_from_ds(\n",
    "            ds=ds,\n",
    "            nc_path=str(nc_path),\n",
    "            lakes_gdf=lakes_gdf,\n",
    "            lake_id_col=lake_id_col,\n",
    "            product=product,\n",
    "            time_label=t,\n",
    "            engine=\"h5netcdf\",\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def try_open_xarray(fp: Path):\n",
    "    \"\"\"Try netcdf4 → h5netcdf → h5py，return (ds or None, engine_used)。\"\"\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "        _ = ds.dims\n",
    "        return ds, \"netcdf4\"\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            ds = xr.open_dataset(fp, engine=\"h5netcdf\", chunks=\"auto\", phony_dims=\"access\")\n",
    "            _ = ds.dims\n",
    "            return ds, \"h5netcdf\"\n",
    "        except Exception as e2:\n",
    "            try:\n",
    "                import h5py\n",
    "                with h5py.File(fp, \"r\") as f:\n",
    "                    pass\n",
    "                print(f\"[WARN] {fp.name} readable by h5py but not by xarray engines (netCDF-4 layout issue?)\")\n",
    "            except Exception as e3:\n",
    "                print(f\"[WARN] {fp.name} not readable even by h5py: {e3}\")\n",
    "            print(f\"[SKIP] {fp.name} → netcdf4:{e1} | h5netcdf:{e2}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4665a61",
   "metadata": {},
   "source": [
    "#### Scale 1: In general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f52fe",
   "metadata": {},
   "source": [
    "##### Monthly\n",
    "\n",
    "Here, we use the **S3B Monthly** data. One month per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f98a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16 16:53:28.500000+00:00</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>2030281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 17:12:11.500000+00:00</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>2204191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-16 16:44:09+00:00</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>1810493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-16 04:52:02+00:00</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>2151149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 17:01:29+00:00</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2586974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time   CI_mean    CI_p90  n_valid\n",
       "0 2024-01-16 16:53:28.500000+00:00  0.000591  0.000472  2030281\n",
       "1 2024-02-15 17:12:11.500000+00:00  0.000539  0.000351  2204191\n",
       "2        2024-03-16 16:44:09+00:00  0.000492  0.000166  1810493\n",
       "3        2024-04-16 04:52:02+00:00  0.000496  0.000540  2151149\n",
       "4        2024-05-16 17:01:29+00:00  0.000500  0.000836  2586974"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "monthly_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO\")\n",
    "out_csv = monthly_dir/\"ci_cyano_monthly_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(monthly_dir.glob(\"S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    with xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\") as ds:\n",
    "        da = clean_ci(ds[\"CI_cyano\"])\n",
    "        arr = np.asarray(da.data)\n",
    "        mask = np.isfinite(arr)\n",
    "        m   = float(np.nanmean(arr[mask])) if mask.any() else np.nan\n",
    "        p90 = float(np.nanquantile(arr[mask], 0.9))    if mask.any() else np.nan\n",
    "        t   = infer_time_label(str(fp), ds, product=\"monthly\")\n",
    "        rows.append({\"time\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                     \"n_valid\": int(mask.sum())})\n",
    "\n",
    "df_mo = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
    "df_mo.to_csv(out_csv, index=False)\n",
    "df_mo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93b8d",
   "metadata": {},
   "source": [
    "##### Daily\n",
    "\n",
    "Here, we use the **S3M Daily** data. One day per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1970169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc not readable even by h5py: Unable to synchronously open file (truncated file: eof = 323158016, sblock->base_addr = 0, stored_eof = 763662772)\n",
      "[SKIP] S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc → netcdf4:[Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240211.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc' | h5netcdf:Unable to synchronously open file (truncated file: eof = 323158016, sblock->base_addr = 0, stored_eof = 763662772)\n",
      "[WARN] S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc not readable even by h5py: Unable to synchronously open file (truncated file: eof = 411697152, sblock->base_addr = 0, stored_eof = 978029795)\n",
      "[SKIP] S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc → netcdf4:[Errno -101] NetCDF: HDF error: '/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20241016.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc' | h5netcdf:Unable to synchronously open file (truncated file: eof = 411697152, sblock->base_addr = 0, stored_eof = 978029795)\n",
      "[OK] saved → /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv (rows=361)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "      <th>src</th>\n",
       "      <th>engine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 13:52:31+00:00</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1024210</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 13:36:49+00:00</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1047655</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03 14:02:06+00:00</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>865219</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04 13:40:26+00:00</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1144789</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05 13:48:41+00:00</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1152641</td>\n",
       "      <td>S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...</td>\n",
       "      <td>netcdf4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   CI_mean   CI_p90  n_valid  \\\n",
       "0 2024-01-01 13:52:31+00:00  0.000432  0.00005  1024210   \n",
       "1 2024-01-02 13:36:49+00:00  0.000274  0.00005  1047655   \n",
       "2 2024-01-03 14:02:06+00:00  0.000368  0.00005   865219   \n",
       "3 2024-01-04 13:40:26+00:00  0.000230  0.00005  1144789   \n",
       "4 2024-01-05 13:48:41+00:00  0.000273  0.00005  1152641   \n",
       "\n",
       "                                                 src   engine  \n",
       "0  S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "1  S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "2  S3M_OLCI_EFRNT.20240103.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "3  S3M_OLCI_EFRNT.20240104.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  \n",
       "4  S3M_OLCI_EFRNT.20240105.L3m.DAY.ILW_CONUS.V5.a...  netcdf4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5py\n",
    "import geopandas as gpd\n",
    "\n",
    "daily_dir = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "out_csv = daily_dir / \"ci_cyano_daily_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    if not looks_like_hdf5(fp):\n",
    "        print(f\"[WARN] Skip (not HDF5 header): {fp.name}\")\n",
    "        continue\n",
    "\n",
    "    ds, eng = try_open_xarray(fp)\n",
    "    if ds is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        da = clean_ci(ds[\"CI_cyano\"])\n",
    "        arr = np.asarray(da.data)\n",
    "        mask = np.isfinite(arr)\n",
    "        m   = float(np.nanmean(arr[mask])) if mask.any() else np.nan\n",
    "        p90 = float(np.nanquantile(arr[mask], 0.9)) if mask.any() else np.nan\n",
    "\n",
    "        t = infer_time_label(str(fp), ds, product=\"daily\")\n",
    "\n",
    "        rows.append({\n",
    "            \"date\":   pd.to_datetime(t),\n",
    "            \"CI_mean\": m,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": int(mask.sum()),\n",
    "            \"src\":     fp.name,\n",
    "            \"engine\":  eng,\n",
    "        })\n",
    "    finally:\n",
    "        ds.close()\n",
    "\n",
    "df_day = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
    "df_day.to_csv(out_csv, index=False)\n",
    "print(f\"[OK] saved → {out_csv} (rows={len(df_day)})\")\n",
    "df_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc325",
   "metadata": {},
   "source": [
    "#### Scale 2: Five Great Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b2c1f",
   "metadata": {},
   "source": [
    "First of all, we get the five Great Lakes out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg features: 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "src = \"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes.gpkg\"\n",
    "gdf = gpd.read_file(src)\n",
    "\n",
    "keep_names = [\"Superior\",\"Michigan\",\"Huron\",\"Erie\",\"Ontario\"]\n",
    "gdf = gpd.read_file(src)\n",
    "gdf5 = gdf[gdf[\"Lake_name\"].str.fullmatch(\"|\".join(keep_names), case=False)].copy()\n",
    "gdf5 = gdf5.dissolve(by=\"Lake_name\", as_index=False).rename(columns={\"Lake_name\":\"lake_name\"})\n",
    "\n",
    "order = [\"Erie\",\"Huron\",\"Michigan\",\"Ontario\",\"Superior\"]\n",
    "gdf5 = gdf5.set_index(\"lake_name\").loc[order].reset_index()\n",
    "\n",
    "# Buffer the shoreline by 300 m, first to equidistant projection, then buffer, then back to 4326\n",
    "gdf5m = gdf5.to_crs(5070)\n",
    "gdf5m[\"geometry\"] = gdf5m.buffer(-300)\n",
    "gdf5m = gdf5m[~gdf5m.geometry.is_empty].copy()\n",
    "gdf5  = gdf5m.to_crs(4326).reset_index(drop=True)\n",
    "\n",
    "gdf5[\"lake_id\"] = [f\"GL-{i+1}\" for i in range(len(gdf5))]\n",
    "\n",
    "out = Path(src).with_name(\"lakes_greatlakes_5poly.gpkg\")\n",
    "gdf5.to_file(out, driver=\"GPKG\")\n",
    "print(\"Saved:\", out, \"features:\", len(gdf5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c8b2d",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3e1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[monthly] saved → /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet  (60 rows)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "def run_monthly(monthly_dir: str,\n",
    "                lakes_fp: str,\n",
    "                lake_id_col: str,\n",
    "                out_parquet: str):\n",
    "    \"\"\"\n",
    "    monthly_dir: Directory containing files like S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS...nc\n",
    "    lakes_fp:    Lake boundaries (gpkg/shp, must be EPSG:4326)\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(monthly_dir).glob(\"S3B_OLCI_EFRNT.*.L3m.MO.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"monthly\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No monthly files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[monthly] saved → {out_parquet}  ({len(df_all)} rows)\")\n",
    "\n",
    "run_monthly(\n",
    "    monthly_dir=\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c23b3",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "def run_daily(daily_dir: str,\n",
    "              lakes_fp: str,\n",
    "              lake_id_col: str,\n",
    "              out_parquet: str):\n",
    "    \"\"\"\n",
    "    daily_dir: Directory containing files like S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS...nc\n",
    "    \"\"\"\n",
    "    daily_dir = Path(daily_dir)\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    files = sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\"))\n",
    "    if not files:\n",
    "        print(f\"[WARN] No daily files found under: {daily_dir}\")\n",
    "        return\n",
    "\n",
    "    for fp in files:\n",
    "        # 1) 文件头快速校验\n",
    "        if not looks_like_hdf5(fp):\n",
    "            print(f\"[WARN] Skip (not HDF5/NetCDF header): {fp.name}\")\n",
    "            continue\n",
    "\n",
    "        # 2) 首选：netcdf4 引擎\n",
    "        try:\n",
    "            df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"daily\")\n",
    "            # 统一列名为 date\n",
    "            if \"time\" in df_one.columns and \"date\" not in df_one.columns:\n",
    "                df_one = df_one.rename(columns={\"time\": \"date\"})\n",
    "            df_one[\"src\"] = fp.name\n",
    "            df_one[\"engine\"] = \"netcdf4\"\n",
    "            out_rows.append(df_one)\n",
    "            continue\n",
    "        except Exception as e1:\n",
    "            print(f\"[WARN] netcdf4 failed on {fp.name}: {e1}\")\n",
    "\n",
    "        # 3) 兜底：h5netcdf 内联提取\n",
    "        try:\n",
    "            df_one = _extract_one_with_h5netcdf(fp, gdf, lake_id_col, product=\"daily\")\n",
    "            if \"time\" in df_one.columns and \"date\" not in df_one.columns:\n",
    "                df_one = df_one.rename(columns={\"time\": \"date\"})\n",
    "            out_rows.append(df_one)\n",
    "        except Exception as e2:\n",
    "            print(f\"[SKIP] {fp.name}: h5netcdf fallback failed → {e2}\")\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No valid daily rows produced.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "    # 基本整理：日期排序、列顺序、小型诊断\n",
    "    keep_cols = [\"lake_id\", \"date\", \"product\", \"CI_mean\", \"CI_p90\", \"n_valid\", \"src\", \"engine\"]\n",
    "    for c in keep_cols:\n",
    "        if c not in df_all.columns:\n",
    "            df_all[c] = np.nan if c not in (\"lake_id\", \"product\", \"src\", \"engine\") else None\n",
    "    df_all = df_all[keep_cols].sort_values([\"lake_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[daily] saved → {out_parquet}  (rows={len(df_all)}, files={len(files)})\")\n",
    "\n",
    "run_daily(\n",
    "    daily_dir=\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773dfe",
   "metadata": {},
   "source": [
    "### Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f4ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/conus_daily_clean.csv\n",
      "[OK] Saved regional QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/conus_monthly_clean.csv\n",
      "\n",
      "[DIAG] per-lake counts:\n",
      " lake_id  n_rows  n_CI_finite  n_valid_pos  qc_valid\n",
      "   GL-1     361          336          336       147\n",
      "   GL-2     361          343          343       122\n",
      "   GL-3     361          346          346       128\n",
      "   GL-4     361          326          326       136\n",
      "   GL-5     361          352          352       114\n",
      "[OK] Saved lake-level QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_daily_clean.parquet\n",
      "[OK] Summary → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_daily_summary.csv\n",
      "\n",
      "[DIAG] per-lake counts:\n",
      " lake_id  n_rows  n_CI_finite  n_valid_pos  qc_valid\n",
      "   GL-1      12           12           12        11\n",
      "   GL-2      12           12           12        10\n",
      "   GL-3      12           12           12        10\n",
      "   GL-4      12           12           12        10\n",
      "   GL-5      12           12           12        11\n",
      "[OK] Saved lake-level QC → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_clean.parquet\n",
      "[OK] Summary → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QC & Completeness checks for NASA ILW Cyanobacteria Index (CI_cyano)\n",
    "- Regional (CONUS) daily & monthly time series\n",
    "- Lake-level (Great Lakes) daily & monthly time series\n",
    "\n",
    "Inputs (already prepared by you):\n",
    "1) /dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\n",
    "2) /dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\n",
    "3) /dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
    "4) /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\n",
    "5) /dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\n",
    "\n",
    "Outputs:\n",
    "- Cleaned regional daily/monthly CSVs with QC flags\n",
    "- Cleaned lake-level daily/monthly Parquet with QC flags and completeness metrics\n",
    "- Simple summary CSVs per product (row counts, missing rates, clipping, etc.)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters (tune as needed)\n",
    "# ----------------------------\n",
    "# Resolution of ILW L3m grid (meters)\n",
    "PIX_RES_M = 300.0\n",
    "\n",
    "# Minimal valid pixel ratio when judged by geometry (n_valid / expected_pixels)\n",
    "MIN_PCT_VALID_GEOM = 0.10   # drop if coverage < 10%\n",
    "\n",
    "# Minimal absolute valid pixels per lake-time\n",
    "MIN_ABS_PIX = 50            # drop very tiny coverage\n",
    "\n",
    "# Empirical coverage threshold relative to best observed coverage for that lake\n",
    "MIN_PCT_VALID_EMP = 0.10    # drop if n_valid < 10% of lake's max observed n_valid\n",
    "\n",
    "# CI cleaning thresholds\n",
    "NEAR_ZERO_THRESHOLD = 5e-5  # drop near-zero values (already applied in earlier step, kept as doc)\n",
    "CLIP_QUANTILE_LOW  = 0.001  # lower clip for outliers\n",
    "CLIP_QUANTILE_HIGH = 0.999  # upper clip for outliers\n",
    "\n",
    "# Interpolation limits for regional (optional smoothing to fill short gaps)\n",
    "INTERP_LIMIT_DAYS = 3\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "P_CONUS_DAILY   = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\")\n",
    "P_CONUS_MONTHLY = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\")\n",
    "P_LAKES_GPKG    = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "P_LAKE_DAILY    = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_daily.parquet\")\n",
    "P_LAKE_MONTHLY  = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/lake_ci_monthly.parquet\")\n",
    "\n",
    "OUT_DIR = Path(\"/dkucc/home/zy166/HAB-forecasting/datasets/processed/qc\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _clip_series_q(s: pd.Series, qlow=CLIP_QUANTILE_LOW, qhigh=CLIP_QUANTILE_HIGH) -> pd.Series:\n",
    "    \"\"\"Clip a numeric series by quantiles; preserve NaNs.\"\"\"\n",
    "    if s.dropna().empty:\n",
    "        return s\n",
    "    lo = s.quantile(qlow)\n",
    "    hi = s.quantile(qhigh)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame,\n",
    "                     prefer_cols=(\"date\", \"time\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust datetime parsing:\n",
    "    - try 'date' then 'time' (or any column that contains these names)\n",
    "    - tolerant to mixed formats; coerce to UTC tz-aware then drop tz\n",
    "    - drop NaT rows\n",
    "    \"\"\"\n",
    "    col = None\n",
    "    for cand in prefer_cols:\n",
    "        if cand in df.columns:\n",
    "            col = cand\n",
    "            break\n",
    "    if col is None:\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if \"date\" in lc or \"time\" in lc or \"datetime\" in lc or \"timestamp\" in lc:\n",
    "                col = c\n",
    "                break\n",
    "    if col is None:\n",
    "        raise ValueError(\"No recognizable datetime column (expect 'date'/'time').\")\n",
    "\n",
    "    dt = pd.to_datetime(df[col], utc=True, format=\"mixed\", errors=\"coerce\")\n",
    "    dt = dt.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = dt\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "    if len(df) < before:\n",
    "        print(f\"[QC] Dropped {before - len(df)} rows with unparseable datetime in column '{col}'\")\n",
    "    return df\n",
    "\n",
    "def _summarize_basic(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Basic counts and missingness summary for quick logging.\"\"\"\n",
    "    out = {\n",
    "        \"tag\": tag,\n",
    "        \"rows\": len(df),\n",
    "        \"n_missing_CI_mean\": int(df[\"CI_mean\"].isna().sum()) if \"CI_mean\" in df.columns else None,\n",
    "        \"n_missing_CI_p90\":  int(df[\"CI_p90\"].isna().sum()) if \"CI_p90\" in df.columns else None,\n",
    "        \"n_missing_n_valid\": int(df[\"n_valid\"].isna().sum()) if \"n_valid\" in df.columns else None,\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "# ----------------------------\n",
    "# QC for regional (CONUS) time series\n",
    "# ----------------------------\n",
    "def qc_conus_timeseries(csv_path: Path, freq: str, out_prefix: str) -> None:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    expected = {\"CI_mean\", \"CI_p90\", \"n_valid\"}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] {csv_path.name} is missing columns: {missing}\")\n",
    "\n",
    "    # robust parse + drop NaT\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # normalize timestamps to start-of-period\n",
    "    if freq.upper() == \"D\":\n",
    "        df[\"date\"] = df[\"date\"].dt.floor(\"D\")\n",
    "    elif freq.upper() == \"MS\":\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    else:\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(freq).dt.start_time\n",
    "\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # build regular index over the observed span\n",
    "    idx = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=freq)\n",
    "    df = df.set_index(\"date\").reindex(idx).rename_axis(\"date\").reset_index()\n",
    "\n",
    "    # short-gap interpolation on CI columns (both directions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].interpolate(\n",
    "                limit=INTERP_LIMIT_DAYS if freq.upper()==\"D\" else 1,\n",
    "                limit_direction=\"both\"\n",
    "            )\n",
    "\n",
    "    # robust quantile clipping\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _clip_series_q(df[c])\n",
    "\n",
    "    if \"n_valid\" in df.columns:\n",
    "        nv_thr = df[\"n_valid\"].quantile(0.05) if df[\"n_valid\"].notna().any() else 0\n",
    "        df[\"qc_low_coverage\"] = (df[\"n_valid\"] < nv_thr).astype(int)\n",
    "\n",
    "    out_csv = OUT_DIR / f\"{out_prefix}_clean.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    _summarize_basic(df, tag=f\"{out_prefix}\").to_csv(OUT_DIR / f\"{out_prefix}_summary.csv\", index=False)\n",
    "    print(f\"[OK] Saved regional QC → {out_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lake geometry → expected pixel count\n",
    "# ----------------------------\n",
    "def compute_expected_pixels_per_lake(lakes_gpkg: Path,\n",
    "                                     inset_m: float = 0.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute expected pixel counts per lake from geometry area, after optional inward buffer\n",
    "    (set inset_m=0 to avoid double-shrinking if polygons are already buffered).\n",
    "    Done in equal-area CRS (EPSG:5070) then back to 4326.\n",
    "\n",
    "    Returns a DataFrame: lake_id, lake_name (if present), expected_pixels_geom, area_m2\n",
    "    \"\"\"\n",
    "    g = gpd.read_file(lakes_gpkg)\n",
    "    cols = list(g.columns)\n",
    "    if \"lake_id\" not in cols:\n",
    "        raise ValueError(\"Expect 'lake_id' in lakes_greatlakes_5poly.gpkg\")\n",
    "\n",
    "    name_col = None\n",
    "    for cand in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "        if cand in cols:\n",
    "            name_col = cand\n",
    "            break\n",
    "\n",
    "    gm = g.to_crs(5070)\n",
    "\n",
    "    if inset_m and inset_m > 0:\n",
    "        gm[\"geometry\"] = gm.buffer(-abs(inset_m))\n",
    "\n",
    "    gm[\"area_m2\"] = gm.geometry.area\n",
    "    gm[\"expected_pixels_geom\"] = (gm[\"area_m2\"] / (PIX_RES_M ** 2)).round().astype(\"Int64\")\n",
    "\n",
    "    df = gm.to_crs(4326)[[\"lake_id\", \"area_m2\", \"expected_pixels_geom\"]].copy()\n",
    "    if name_col:\n",
    "        df[name_col] = g[name_col].values\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# QC for lake-level time series\n",
    "# ----------------------------\n",
    "def _safe_nanmedian(arr) -> float:\n",
    "    \"\"\"Return NaN if all values are NaN or array empty; otherwise nanmedian.\"\"\"\n",
    "    a = pd.Series(arr).astype(float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    return float(np.nanmedian(a)) if len(a) else float(\"nan\")\n",
    "\n",
    "def _coerce_n_valid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) 如果已有 n_valid 且存在非零值，直接返回\n",
    "    if \"n_valid\" in df.columns and (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        return df\n",
    "\n",
    "    # 2) 常见别名兜底\n",
    "    for alias in [\"valid_count\", \"count_valid\", \"num_valid\", \"n\", \"count\"]:\n",
    "        if alias in df.columns and (df[alias].fillna(0) > 0).any():\n",
    "            df = df.rename(columns={alias: \"n_valid\"})\n",
    "            return df\n",
    "\n",
    "    # 3) 若 n_valid 不存在或全为 0 → 改成 NaN，避免误触发覆盖率判定\n",
    "    if \"n_valid\" not in df.columns or not (df[\"n_valid\"].fillna(0) > 0).any():\n",
    "        df[\"n_valid\"] = np.nan\n",
    "        print(\"[WARN] n_valid is missing or all zeros in lake parquet; \"\n",
    "              \"set n_valid = NaN so coverage flags become NA.\")\n",
    "    return df\n",
    "\n",
    "def qc_lake_timeseries(parquet_path: Path, lakes_gpkg: Path, out_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    QC for lake-level CI time series (daily or monthly parquet).\n",
    "    Adds:\n",
    "      - expected_pixels based on lake geometry\n",
    "      - pct_valid_geom = n_valid / expected_pixels\n",
    "      - pct_valid_emp = n_valid / max(n_valid) per lake\n",
    "      - QC flags: low coverage (geom/emp), tiny absolute pixels\n",
    "    Outputs cleaned Parquet + summary CSV.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = _coerce_n_valid(df)\n",
    "\n",
    "    # Robust datetime parse: prefer 'date', then 'time'\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # Ensure single datetime column named 'date'\n",
    "    if \"time\" in df.columns and \"date\" in df.columns:\n",
    "        df = df.drop(columns=[\"time\"])\n",
    "    elif \"time\" in df.columns and \"date\" not in df.columns:\n",
    "        df = df.rename(columns={\"time\": \"date\"})\n",
    "\n",
    "    # Drop duplicated columns if any\n",
    "    if df.columns.duplicated().any():\n",
    "        dup = df.columns[df.columns.duplicated()].tolist()\n",
    "        print(f\"[QC] Dropping duplicated columns: {dup}\")\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "    # Ensure a 'product' column exists\n",
    "    if \"product\" not in df.columns:\n",
    "        df[\"product\"] = \"unknown\"\n",
    "\n",
    "    # Expected pixels per lake from geometry (no extra inset here)\n",
    "    geom_df = compute_expected_pixels_per_lake(lakes_gpkg, inset_m=0.0)\n",
    "    name_col = \"lake_name\" if \"lake_name\" in geom_df.columns else (\"Lake_name\" if \"Lake_name\" in geom_df.columns else None)\n",
    "\n",
    "    # Merge expected pixels\n",
    "    df = df.merge(geom_df, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "    # Empirical max n_valid per lake (skip zeros/NaNs)\n",
    "    if \"n_valid\" in df.columns:\n",
    "        emp_max = (\n",
    "            df.loc[df[\"n_valid\"].fillna(0) > 0]\n",
    "              .groupby(\"lake_id\")[\"n_valid\"]\n",
    "              .max()\n",
    "              .rename(\"empiric_n_valid_max\")\n",
    "        )\n",
    "        df = df.merge(emp_max, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "        # Coverage metrics\n",
    "        df[\"pct_valid_geom\"] = df[\"n_valid\"] / df[\"expected_pixels_geom\"]\n",
    "        df[\"pct_valid_emp\"]  = df[\"n_valid\"] / df[\"empiric_n_valid_max\"]\n",
    "\n",
    "        for c in (\"pct_valid_geom\", \"pct_valid_emp\"):\n",
    "            df.loc[~np.isfinite(df[c]), c] = np.nan\n",
    "\n",
    "        # Coverage flags\n",
    "        df[\"qc_low_cov_geom\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_geom = df[\"pct_valid_geom\"].notna()\n",
    "        df.loc[ok_geom, \"qc_low_cov_geom\"] = (df.loc[ok_geom, \"pct_valid_geom\"] < MIN_PCT_VALID_GEOM).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_low_cov_emp\"] = pd.Series(pd.NA, index=df.index, dtype=\"Int64\")\n",
    "        ok_emp = df[\"pct_valid_emp\"].notna()\n",
    "        df.loc[ok_emp, \"qc_low_cov_emp\"] = (df.loc[ok_emp, \"pct_valid_emp\"] < MIN_PCT_VALID_EMP).astype(\"Int64\")\n",
    "\n",
    "        df[\"qc_tiny_abs_pix\"] = (pd.to_numeric(df[\"n_valid\"], errors=\"coerce\") < MIN_ABS_PIX).astype(\"Int64\")\n",
    "    else:\n",
    "        for c in [\"pct_valid_geom\", \"pct_valid_emp\", \"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # Clip CI columns lake-wise (robust to per-lake distributions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df.groupby(\"lake_id\", group_keys=False)[c].apply(\n",
    "                lambda s: _clip_series_q(s, CLIP_QUANTILE_LOW, CLIP_QUANTILE_HIGH)\n",
    "            )\n",
    "\n",
    "    # Consolidated validity flag\n",
    "    cov_flags = [\"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]\n",
    "    for c in cov_flags:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    df[\"qc_is_valid\"] = 1  # start as valid\n",
    "    for c in cov_flags:\n",
    "        df.loc[df[c] == 1, \"qc_is_valid\"] = 0\n",
    "\n",
    "    # rows with no coverage info → fall back to \"CI_mean is finite\"\n",
    "    rows_no_cov = df[cov_flags].isna().all(axis=1)\n",
    "    ci_ok = pd.to_numeric(df.get(\"CI_mean\"), errors=\"coerce\")\n",
    "    ci_ok = ci_ok.apply(lambda x: int(np.isfinite(x)))\n",
    "    df.loc[rows_no_cov, \"qc_is_valid\"] = ci_ok.loc[rows_no_cov].values\n",
    "\n",
    "    # Optional quick diagnostics (helps when a lake looks empty in plots)\n",
    "    diag = (\n",
    "        df.assign(CI_mean_finite = pd.to_numeric(df[\"CI_mean\"], errors=\"coerce\").apply(np.isfinite))\n",
    "          .groupby(\"lake_id\")\n",
    "          .agg(n_rows=(\"lake_id\",\"size\"),\n",
    "               n_CI_finite=(\"CI_mean_finite\",\"sum\"),\n",
    "               n_valid_pos=(\"n_valid\", lambda s: int((pd.to_numeric(s, errors=\"coerce\")>0).sum())),\n",
    "               qc_valid=(\"qc_is_valid\",\"sum\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    print(\"\\n[DIAG] per-lake counts:\\n\", diag.to_string(index=False))\n",
    "\n",
    "    # Save cleaned parquet\n",
    "    out_pq = OUT_DIR / f\"{out_prefix}_clean.parquet\"\n",
    "    df.to_parquet(out_pq, index=False)\n",
    "\n",
    "    # Build simple summary per-lake\n",
    "    summary_rows = []\n",
    "    for lid, g in df.groupby(\"lake_id\"):\n",
    "        n_rows = len(g)\n",
    "        n_valid_rows = int((g[\"qc_is_valid\"] == 1).sum())\n",
    "        frac_valid = n_valid_rows / n_rows if n_rows else 0.0\n",
    "        row = {\n",
    "            \"lake_id\": lid,\n",
    "            \"rows\": n_rows,\n",
    "            \"rows_valid\": n_valid_rows,\n",
    "            \"frac_valid\": round(frac_valid, 4),\n",
    "            \"pct_valid_geom_med\": _safe_nanmedian(g[\"pct_valid_geom\"]) if \"pct_valid_geom\" in g.columns else np.nan,\n",
    "            \"pct_valid_emp_med\":  _safe_nanmedian(g[\"pct_valid_emp\"])  if \"pct_valid_emp\"  in g.columns else np.nan,\n",
    "        }\n",
    "        if name_col and name_col in g.columns:\n",
    "            row[\"lake_name\"] = g[name_col].iloc[0]\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"lake_id\")\n",
    "    out_summary = OUT_DIR / f\"{out_prefix}_summary.csv\"\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved lake-level QC → {out_pq}\")\n",
    "    print(f\"[OK] Summary → {out_summary}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Regional time series QC\n",
    "    if P_CONUS_DAILY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_DAILY,   freq=\"D\",  out_prefix=\"conus_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_DAILY}\")\n",
    "\n",
    "    if P_CONUS_MONTHLY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_MONTHLY, freq=\"MS\", out_prefix=\"conus_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_MONTHLY}\")\n",
    "\n",
    "    # 2) Lake-level QC (Great Lakes)\n",
    "    if P_LAKE_DAILY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_DAILY,  P_LAKES_GPKG, out_prefix=\"greatlakes_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_DAILY}\")\n",
    "\n",
    "    if P_LAKE_MONTHLY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_MONTHLY, P_LAKES_GPKG, out_prefix=\"greatlakes_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_MONTHLY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254a9d3",
   "metadata": {},
   "source": [
    "### Dataset Visualization: ILW Monthly & Daily for Five Great Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3173f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/conus_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/conus_monthly_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-1_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-2_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-3_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-4_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-5_daily_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/greatlakes_daily_facets.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/greatlakes_daily_validity_heatmap.png\n",
      "[INFO] Filtered by qc_is_valid==1: 60 -> 52 rows\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-1_monthly_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-2_monthly_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-3_monthly_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-4_monthly_timeseries.png\n",
      "[OK] Saved /dkucc/home/zy166/HAB-forecasting/visualization/ILW/lake_GL-5_monthly_timeseries.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualization for ILW CI_cyano after QC.\n",
    "Generates:\n",
    "- CONUS daily & monthly time-series plots\n",
    "- Per-lake (Great Lakes) daily & monthly time series with coverage\n",
    "- Faceted (5-lake) small multiples\n",
    "- Validity heatmap (lake x date)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "ROOT = Path(\"/dkucc/home/zy166/HAB-forecasting\")\n",
    "QC_DIR = ROOT / \"datasets/processed/qc\"\n",
    "FIG_DIR = ROOT / \"visualization/ILW\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P_CONUS_DAILY_CLEAN   = QC_DIR / \"conus_daily_clean.csv\"\n",
    "P_CONUS_MONTHLY_CLEAN = QC_DIR / \"conus_monthly_clean.csv\"\n",
    "P_GL_DAILY_CLEAN      = QC_DIR / \"greatlakes_daily_clean.parquet\"\n",
    "P_GL_MONTHLY_CLEAN    = QC_DIR / \"greatlakes_monthly_clean.parquet\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _rolling(s, win=7):\n",
    "    \"\"\"Simple rolling mean with center window, preserving NaNs at edges.\"\"\"\n",
    "    try:\n",
    "        return s.rolling(win, center=True, min_periods=max(1, win // 2)).mean()\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def _nice_ax(ax, title=None, xlabel=None, ylabel=None, grid=True):\n",
    "    \"\"\"Minimal axis styling.\"\"\"\n",
    "    if title:  ax.set_title(title, fontsize=12)\n",
    "    if xlabel: ax.set_xlabel(xlabel)\n",
    "    if ylabel: ax.set_ylabel(ylabel)\n",
    "    if grid:   ax.grid(True, alpha=0.25, linestyle=\"--\", linewidth=0.8)\n",
    "\n",
    "def _lake_name_map(df):\n",
    "    \"\"\"Resolve a friendly lake name column.\"\"\"\n",
    "    for c in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# 1) CONUS time series\n",
    "# -----------------------------\n",
    "def plot_conus_timeseries():\n",
    "    \"\"\"Plot CONUS daily & monthly CI time series.\"\"\"\n",
    "    if not P_CONUS_DAILY_CLEAN.exists() and not P_CONUS_MONTHLY_CLEAN.exists():\n",
    "        print(\"[SKIP] No CONUS files found.\")\n",
    "        return\n",
    "\n",
    "    # Daily\n",
    "    if P_CONUS_DAILY_CLEAN.exists():\n",
    "        d = pd.read_csv(P_CONUS_DAILY_CLEAN, parse_dates=[\"date\"])\n",
    "        d = d.sort_values(\"date\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        if \"CI_mean\" in d.columns:\n",
    "            ax.plot(d[\"date\"], d[\"CI_mean\"], linewidth=1.0, label=\"CI_mean (daily)\")\n",
    "            ax.plot(d[\"date\"], _rolling(d[\"CI_mean\"], 7), linewidth=2.0, label=\"CI_mean 7D MA\")\n",
    "        if \"CI_p90\" in d.columns:\n",
    "            ax.plot(d[\"date\"], _rolling(d[\"CI_p90\"], 7), linewidth=1.5, linestyle=\":\", label=\"CI_p90 7D MA\")\n",
    "        _nice_ax(ax, title=\"CONUS — Daily CI_cyano (with 7D moving average)\", xlabel=\"Date\", ylabel=\"CI\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out = FIG_DIR / \"conus_daily_timeseries.png\"\n",
    "        fig.savefig(out, dpi=180)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved\", out)\n",
    "\n",
    "    # Monthly\n",
    "    if P_CONUS_MONTHLY_CLEAN.exists():\n",
    "        m = pd.read_csv(P_CONUS_MONTHLY_CLEAN, parse_dates=[\"date\"])\n",
    "        m = m.sort_values(\"date\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        if \"CI_mean\" in m.columns:\n",
    "            ax.plot(m[\"date\"], m[\"CI_mean\"], marker=\"o\", linewidth=2.0, label=\"CI_mean (monthly)\")\n",
    "        if \"CI_p90\" in m.columns:\n",
    "            ax.plot(m[\"date\"], m[\"CI_p90\"], marker=\"s\", linewidth=1.8, linestyle=\"--\", label=\"CI_p90 (monthly)\")\n",
    "        _nice_ax(ax, title=\"CONUS — Monthly CI_cyano\", xlabel=\"Month\", ylabel=\"CI\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out = FIG_DIR / \"conus_monthly_timeseries.png\"\n",
    "        fig.savefig(out, dpi=180)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved\", out)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Great Lakes time series\n",
    "# -----------------------------\n",
    "def plot_lake_timeseries_daily():\n",
    "    \"\"\"Per-lake time series with coverage as secondary axis.\"\"\"\n",
    "    if not P_GL_DAILY_CLEAN.exists():\n",
    "        print(\"[SKIP] No Great Lakes daily parquet.\")\n",
    "        return\n",
    "    df = pd.read_parquet(P_GL_DAILY_CLEAN)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    name_col = _lake_name_map(df)\n",
    "    lakes = sorted(df[\"lake_id\"].unique().tolist())\n",
    "\n",
    "    for lid in lakes:\n",
    "        g = df[df[\"lake_id\"] == lid].sort_values(\"date\")\n",
    "        title = f\"Lake {g[name_col].iloc[0] if name_col else lid} — Daily CI & Coverage\"\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "        # CI curves\n",
    "        has_mean = \"CI_mean\" in g.columns and g[\"CI_mean\"].notna().any()\n",
    "        has_p90  = \"CI_p90\" in g.columns and g[\"CI_p90\"].notna().any()\n",
    "        if not (has_mean or has_p90):\n",
    "            ax1.text(0.5, 0.5, \"no finite CI values\", ha=\"center\", va=\"center\", transform=ax1.transAxes)\n",
    "        if has_mean:\n",
    "            ax1.plot(g[\"date\"], g[\"CI_mean\"], linewidth=1.0, label=\"CI_mean\")\n",
    "            ax1.plot(g[\"date\"], _rolling(g[\"CI_mean\"], 7), linewidth=2.0, label=\"CI_mean 7D MA\")\n",
    "        if has_p90:\n",
    "            ax1.plot(g[\"date\"], _rolling(g[\"CI_p90\"], 7), linewidth=1.5, linestyle=\":\", label=\"CI_p90 7D MA\")\n",
    "        _nice_ax(ax1, title=title, xlabel=\"Date\", ylabel=\"CI\")\n",
    "        # Coverage on twin axis\n",
    "        ax2 = ax1.twinx()\n",
    "        if \"pct_valid_geom\" in g.columns and g[\"pct_valid_geom\"].notna().any():\n",
    "            ax2.plot(g[\"date\"], g[\"pct_valid_geom\"], linewidth=1.0, alpha=0.6, label=\"Coverage (geom)\")\n",
    "            ax2.set_ylabel(\"Coverage ratio\")\n",
    "            ax2.set_ylim(0, 1.05)\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "        fig.tight_layout()\n",
    "        out = FIG_DIR / f\"lake_{lid}_daily_timeseries.png\"\n",
    "        fig.savefig(out, dpi=180)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved\", out)\n",
    "\n",
    "def plot_lake_timeseries_monthly():\n",
    "    \"\"\"Plot monthly CI time series for each Great Lake with robust checks.\"\"\"\n",
    "    if not P_GL_MONTHLY_CLEAN.exists():\n",
    "        print(\"[SKIP] No Great Lakes monthly parquet.\")\n",
    "        return\n",
    "    df = pd.read_parquet(P_GL_MONTHLY_CLEAN)\n",
    "    if \"date\" not in df.columns and \"time\" in df.columns:\n",
    "        df = df.rename(columns={\"time\": \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    if \"qc_is_valid\" in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df[\"qc_is_valid\"] == 1]\n",
    "        print(f\"[INFO] Filtered by qc_is_valid==1: {before} -> {len(df)} rows\")\n",
    "\n",
    "    has_mean = \"CI_mean\" in df.columns\n",
    "    has_p90 = \"CI_p90\" in df.columns\n",
    "    if not (has_mean or has_p90):\n",
    "        print(\"[WARN] Neither CI_mean nor CI_p90 present.\")\n",
    "        return\n",
    "\n",
    "    name_col = _lake_name_map(df)\n",
    "    lakes = sorted(df[\"lake_id\"].dropna().unique().tolist())\n",
    "\n",
    "    for lid in lakes:\n",
    "        g = df[df[\"lake_id\"] == lid].sort_values(\"date\")\n",
    "        title_name = g[name_col].iloc[0] if name_col and len(g[name_col].dropna()) else lid\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        plotted = False\n",
    "        if has_mean and g[\"CI_mean\"].notna().any():\n",
    "            ax.plot(g[\"date\"], g[\"CI_mean\"], marker=\"o\", linewidth=2.0, label=\"CI_mean (monthly)\")\n",
    "            plotted = True\n",
    "        if has_p90 and g[\"CI_p90\"].notna().any():\n",
    "            ax.plot(g[\"date\"], g[\"CI_p90\"], marker=\"s\", linewidth=1.6, linestyle=\"--\", label=\"CI_p90 (monthly)\")\n",
    "            plotted = True\n",
    "        if not plotted:\n",
    "            ax.text(0.5, 0.5, \"no finite CI values\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        _nice_ax(ax, title=f\"Lake {title_name} — Monthly CI\", xlabel=\"Month\", ylabel=\"CI\")\n",
    "        if plotted:\n",
    "            ax.legend()\n",
    "        fig.tight_layout()\n",
    "        out = FIG_DIR / f\"lake_{lid}_monthly_timeseries.png\"\n",
    "        fig.savefig(out, dpi=180)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved\", out)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Faceted small multiples & validity heatmap\n",
    "# -----------------------------\n",
    "def plot_lake_facets_daily():\n",
    "    \"\"\"Five-lake small multiples (CI_mean 7D MA + coverage shading).\"\"\"\n",
    "    if not P_GL_DAILY_CLEAN.exists():\n",
    "        print(\"[SKIP] No Great Lakes daily parquet.\")\n",
    "        return\n",
    "    df = pd.read_parquet(P_GL_DAILY_CLEAN)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    name_col = _lake_name_map(df)\n",
    "    desired = [\"Superior\", \"Michigan\", \"Huron\", \"Erie\", \"Ontario\"]\n",
    "    if name_col:\n",
    "        order = []\n",
    "        for nm in desired:\n",
    "            subset = df[df[name_col].str.fullmatch(nm, case=False, na=False)]\n",
    "            if not subset.empty:\n",
    "                order.extend(subset[\"lake_id\"].unique().tolist())\n",
    "        lakes = order or sorted(df[\"lake_id\"].unique().tolist())[:5]\n",
    "    else:\n",
    "        lakes = sorted(df[\"lake_id\"].unique().tolist())[:5]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 9))\n",
    "    axes = axes.ravel()\n",
    "    for i, lid in enumerate(lakes[:5]):\n",
    "        ax = axes[i]\n",
    "        g = df[df[\"lake_id\"] == lid].sort_values(\"date\")\n",
    "        label = g[name_col].iloc[0] if name_col else str(lid)\n",
    "        if \"pct_valid_geom\" in g.columns:\n",
    "            ycov = (g[\"pct_valid_geom\"].clip(0, 1.0) * (g[\"CI_mean\"].max() * 0.2)).fillna(0.0)\n",
    "            ax.fill_between(g[\"date\"], 0, ycov, alpha=0.15, step=\"pre\")\n",
    "        if \"CI_mean\" in g.columns:\n",
    "            ax.plot(g[\"date\"], _rolling(g[\"CI_mean\"], 7), linewidth=2.0, label=\"CI_mean 7D MA\")\n",
    "        if \"CI_p90\" in g.columns:\n",
    "            ax.plot(g[\"date\"], _rolling(g[\"CI_p90\"], 7), linewidth=1.2, linestyle=\"--\", label=\"CI_p90 7D MA\")\n",
    "        _nice_ax(ax, title=label, xlabel=None, ylabel=\"CI\")\n",
    "        if i == 0:\n",
    "            ax.legend(fontsize=9)\n",
    "    for j in range(len(lakes), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    fig.suptitle(\"Great Lakes — Daily CI (7D MA) with coverage shading\", fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0.02, 1, 0.97])\n",
    "    out = FIG_DIR / \"greatlakes_daily_facets.png\"\n",
    "    fig.savefig(out, dpi=180)\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Saved\", out)\n",
    "\n",
    "def plot_validity_heatmap_daily():\n",
    "    \"\"\"Heatmap of qc_is_valid by (lake x date).\"\"\"\n",
    "    if not P_GL_DAILY_CLEAN.exists():\n",
    "        print(\"[SKIP] No Great Lakes daily parquet.\")\n",
    "        return\n",
    "    df = pd.read_parquet(P_GL_DAILY_CLEAN)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    name_col = _lake_name_map(df)\n",
    "    if name_col:\n",
    "        id2name = (df.dropna(subset=[name_col])\n",
    "                     .drop_duplicates(subset=[\"lake_id\"])[[\"lake_id\", name_col]]\n",
    "                     .set_index(\"lake_id\")[name_col].to_dict())\n",
    "    else:\n",
    "        id2name = {}\n",
    "    mat = df.pivot_table(index=\"lake_id\", columns=\"date\", values=\"qc_is_valid\", aggfunc=\"max\")\n",
    "    if id2name:\n",
    "        mat.index = [f\"{lid} - {id2name.get(lid, '')}\" for lid in mat.index]\n",
    "    mat = mat.sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(12, 3 + 0.35 * len(mat)))\n",
    "    im = ax.imshow(mat.values, aspect=\"auto\", interpolation=\"nearest\", vmin=0, vmax=1)\n",
    "    ax.set_yticks(np.arange(len(mat)))\n",
    "    ax.set_yticklabels(mat.index, fontsize=9)\n",
    "    dates = mat.columns.to_pydatetime()\n",
    "    step = max(1, len(dates) // 12)\n",
    "    xticks = np.arange(0, len(dates), step)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([dates[i].strftime(\"%Y-%m-%d\") for i in xticks], rotation=45, ha=\"right\", fontsize=8)\n",
    "    _nice_ax(ax, title=\"Great Lakes — Daily validity (qc_is_valid) heatmap\", xlabel=\"Date\", ylabel=\"Lake\")\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    cbar.set_label(\"qc_is_valid (1=valid, 0=invalid)\", rotation=270, labelpad=12)\n",
    "    fig.tight_layout()\n",
    "    out = FIG_DIR / \"greatlakes_daily_validity_heatmap.png\"\n",
    "    fig.savefig(out, dpi=180)\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Saved\", out)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    plot_conus_timeseries()\n",
    "    plot_lake_timeseries_daily()\n",
    "    plot_lake_facets_daily()\n",
    "    plot_validity_heatmap_daily()\n",
    "    plot_lake_timeseries_monthly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fdb30",
   "metadata": {},
   "source": [
    "## Dataset: Daymet v4 Daily Surface Weather Data (ORNL DAAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50aaa9",
   "metadata": {},
   "source": [
    "Daymet: **Daily** Surface Weather Data on a 1-km Grid for North America, Version 4 R1\n",
    "- Dataset Description: https://www.earthdata.nasa.gov/data/catalog/ornl-cloud-daymet-daily-v4r1-2129-4.1.\n",
    "- More specifically, we need the data that fits into **North America (`na`)**, they are:\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_tmin_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_vp_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_prcp_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_srad_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_tmax_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_dayl_2024.nc\n",
    "    - https://data.ornldaac.earthdata.nasa.gov/protected/daymet/Daymet_Daily_V4R1/data/daymet_v4_daily_na_swe_2024.nc\n",
    "\n",
    "Daymet: **Monthly** Climate Summaries on a 1-km Grid for North America, Version 4 R1\n",
    "- Dataset Description: https://www.earthdata.nasa.gov/data/catalog/ornl-cloud-daymet-monthly-v4r1-2131-4.1.\n",
    "- More specifically, we need the data that fits into **North America (`na`)**, they are:\n",
    "    - (Links coming soon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13671ac",
   "metadata": {},
   "source": [
    "### Daymet Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ad628",
   "metadata": {},
   "source": [
    "Preliminary settings for the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lake_id lake_name                                           geometry\n",
      "0    GL-1      Erie  MULTIPOLYGON (((-83.46202 41.74248, -83.46203 ...\n",
      "1    GL-2     Huron  MULTIPOLYGON (((-84.7498 45.83154, -84.74973 4...\n",
      "2    GL-3  Michigan  MULTIPOLYGON (((-88.03947 44.56444, -88.03912 ...\n",
      "3    GL-4   Ontario  MULTIPOLYGON (((-79.88425 43.27778, -79.88423 ...\n",
      "4    GL-5  Superior  MULTIPOLYGON (((-92.19317 46.6747, -92.19318 4...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from shapely import geometry\n",
    "from shapely import vectorized  # shapely.vectorized.contains\n",
    "\n",
    "# paths\n",
    "ROOT = Path(\"/dkucc/home/zy166/HAB-forecasting\")\n",
    "\n",
    "GPKG = ROOT / \"datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\"\n",
    "DAYMET_DIR = ROOT / \"datasets/Daymet/2024_north_america_daily\"\n",
    "OUT_DAYMET_DAILY = ROOT / \"datasets/Daymet/daymet_glakes_daily.parquet\"\n",
    "\n",
    "# lakes vector, reproject to EPSG:4326 (lat/lon)\n",
    "lakes = gpd.read_file(GPKG)\n",
    "lakes = lakes.to_crs(4326)\n",
    "\n",
    "if \"lake_id\" not in lakes.columns:\n",
    "    raise ValueError(\"Expect 'lake_id' in lakes_greatlakes_5poly.gpkg\")\n",
    "\n",
    "# find a name column, not required\n",
    "name_col = None\n",
    "for cand in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "    if cand in lakes.columns:\n",
    "        name_col = cand\n",
    "        break\n",
    "if name_col is None:\n",
    "    lakes[\"lake_name\"] = lakes[\"lake_id\"]\n",
    "    name_col = \"lake_name\"\n",
    "\n",
    "lakes = lakes[[\"lake_id\", name_col, \"geometry\"]]\n",
    "\n",
    "print(lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19464b83",
   "metadata": {},
   "source": [
    "Build the \"mask\" for each lake in the Daymet map\n",
    "\n",
    "- There are `lat(y,x)` and `lon(y,x)` in the `.nc` file of Daymet datasets.\n",
    "- For each polygon of the lake, use `shapely.vectorized.contains(geom, lon, lat)` to get a `y × x` boolean mask.\n",
    "- Therefore, for eacvh variable (e.g., min/tmax/...), we can use `da.where(mask)` to take the mean of `(y,x)` easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c158faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455028/3285875099.py:5: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds_grid = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/3285875099.py:5: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds_grid = xr.open_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 93GB\n",
      "Dimensions:                  (time: 365, nv: 2, y: 8075, x: 7814)\n",
      "Coordinates:\n",
      "  * time                     (time) datetime64[ns] 3kB 2024-01-01T12:00:00 .....\n",
      "  * y                        (y) float32 32kB 4.984e+06 4.983e+06 ... -3.09e+06\n",
      "  * x                        (x) float32 31kB -4.56e+06 -4.559e+06 ... 3.253e+06\n",
      "    lat                      (y, x) float32 252MB dask.array<chunksize=(1000, 1000), meta=np.ndarray>\n",
      "    lon                      (y, x) float32 252MB dask.array<chunksize=(1000, 1000), meta=np.ndarray>\n",
      "Dimensions without coordinates: nv\n",
      "Data variables:\n",
      "    yearday                  (time) int16 730B dask.array<chunksize=(365,), meta=np.ndarray>\n",
      "    time_bnds                (time, nv) datetime64[ns] 6kB dask.array<chunksize=(365, 2), meta=np.ndarray>\n",
      "    lambert_conformal_conic  int16 2B ...\n",
      "    tmin                     (time, y, x) float32 92GB dask.array<chunksize=(365, 1000, 1000), meta=np.ndarray>\n",
      "Attributes:\n",
      "    start_year:        2024\n",
      "    source:            Daymet Software Version 4.0\n",
      "    Version_software:  Daymet Software Version 4.0\n",
      "    Version_data:      Daymet Data Version 4.0\n",
      "    Conventions:       CF-1.6\n",
      "    citation:          Please see http://daymet.ornl.gov/ for current Daymet ...\n",
      "    references:        Please see http://daymet.ornl.gov/ for current informa...\n",
      "[INFO] Daymet grid shape: ny=8075, nx=7814\n",
      "[INFO] build mask for lake GL-1 (Erie) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455028/3285875099.py:36: DeprecationWarning: The 'shapely.vectorized.contains' function is deprecated and will be removed a future version. Use 'shapely.contains_xy' instead (available since shapely 2.0.0).\n",
      "  mask_bool = vectorized.contains(geom, lon, lat)  # shape: (ny, nx) bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] build mask for lake GL-2 (Huron) ...\n",
      "[INFO] build mask for lake GL-3 (Michigan) ...\n",
      "[INFO] build mask for lake GL-4 (Ontario) ...\n",
      "[INFO] build mask for lake GL-5 (Superior) ...\n",
      "[OK] built masks for 5 lakes\n"
     ]
    }
   ],
   "source": [
    "# Use one file (e.g., tmin) to get the lat/lon grid.\n",
    "f_grid = DAYMET_DIR / \"daymet_v4_daily_na_tmin_2024.nc\"\n",
    "\n",
    "# Use dask chunk to reduce the memory pressure (about 1km grid of North America)\n",
    "ds_grid = xr.open_dataset(\n",
    "    f_grid,\n",
    "    chunks={\"time\": 365, \"y\": 1000, \"x\": 1000},\n",
    ")\n",
    "\n",
    "print(ds_grid)\n",
    "\n",
    "# According to the Daymet documentation, there are usually `lat(y,x)` and `lon(y,x)`\n",
    "# If the names are not exactly these two, you can print ds_grid.data_vars / ds_grid.coords to see the exact names\n",
    "lat = ds_grid[\"lat\"].values  # shape: (ny, nx)\n",
    "lon = ds_grid[\"lon\"].values  # shape: (ny, nx)\n",
    "\n",
    "ny, nx = lat.shape\n",
    "print(f\"[INFO] Daymet grid shape: ny={ny}, nx={nx}\")\n",
    "\n",
    "# Prepare a mask DataArray for each lake\n",
    "lake_masks = {}  # lake_id -> xr.DataArray(bool[y,x])\n",
    "\n",
    "for _, row in lakes.iterrows():\n",
    "    lake_id   = row[\"lake_id\"]\n",
    "    lake_name = row[name_col]\n",
    "    geom      = row.geometry\n",
    "\n",
    "    # Some MultiPolygon / topology may have problems, buffer(0)一下\n",
    "    if not isinstance(geom, geometry.base.BaseGeometry):\n",
    "        geom = geometry.shape(geom)\n",
    "    geom = geom.buffer(0)\n",
    "\n",
    "    print(f\"[INFO] build mask for lake {lake_id} ({lake_name}) ...\")\n",
    "\n",
    "    # shapely.vectorized.contains expects (geom, x, y) = (polygon, lon, lat)\n",
    "    mask_bool = vectorized.contains(geom, lon, lat)  # shape: (ny, nx) bool\n",
    "\n",
    "    if not mask_bool.any():\n",
    "        print(f\"[WARN] mask for lake {lake_id} has no True pixels, check geometry/CRS\")\n",
    "    lake_masks[lake_id] = xr.DataArray(\n",
    "        mask_bool,\n",
    "        dims=(\"y\", \"x\"),\n",
    "        coords={\"y\": ds_grid[\"y\"], \"x\": ds_grid[\"x\"]},\n",
    "        name=f\"mask_{lake_id}\",\n",
    "    )\n",
    "\n",
    "print(f\"[OK] built masks for {len(lake_masks)} lakes\")\n",
    "ds_grid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7955",
   "metadata": {},
   "source": [
    "Take the daily average of each variable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map variable names to file names\n",
    "VAR_FILES = {\n",
    "    \"tmin\": \"daymet_v4_daily_na_tmin_2024.nc\",\n",
    "    \"tmax\": \"daymet_v4_daily_na_tmax_2024.nc\",\n",
    "    \"prcp\": \"daymet_v4_daily_na_prcp_2024.nc\",\n",
    "    \"srad\": \"daymet_v4_daily_na_srad_2024.nc\",\n",
    "    \"vp\":   \"daymet_v4_daily_na_vp_2024.nc\",\n",
    "    \"dayl\": \"daymet_v4_daily_na_dayl_2024.nc\",\n",
    "}\n",
    "\n",
    "def compute_lake_daily_for_var(var_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the daily mean time series of each lake for a Daymet variable (e.g., tmin).\n",
    "    Return columns: ['lake_id', 'date', var_name]\n",
    "    \"\"\"\n",
    "    fpath = DAYMET_DIR / VAR_FILES[var_name]\n",
    "    print(f\"[INFO] open {var_name} from {fpath}\")\n",
    "\n",
    "    ds = xr.open_dataset(\n",
    "        fpath,\n",
    "        chunks={\"time\": 30, \"y\": 1000, \"x\": 1000},\n",
    "    )\n",
    "    # The variable name is usually the same as the file name, if not, change it here\n",
    "    da = ds[var_name]  # dims: time, y, x\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, row in lakes.iterrows():\n",
    "        lake_id   = row[\"lake_id\"]\n",
    "        lake_name = row[name_col]\n",
    "        mask_da   = lake_masks[lake_id]  # bool[y,x]\n",
    "\n",
    "        print(f\"[INFO] compute {var_name} daily mean for lake {lake_id} ({lake_name}) ...\")\n",
    "\n",
    "        # broadcast mask to (time, y, x), only keep the pixels inside the lake\n",
    "        masked = da.where(mask_da)\n",
    "\n",
    "        # Take the mean of the spatial dimensions, get the time series\n",
    "        series = masked.mean(dim=(\"y\", \"x\"), skipna=True)  # DataArray(time)\n",
    "\n",
    "        df = series.to_dataframe(name=var_name).reset_index()  # columns: ['time', var_name]\n",
    "        df[\"lake_id\"]   = lake_id\n",
    "        df[\"lake_name\"] = lake_name\n",
    "        rows.append(df)\n",
    "\n",
    "    ds.close()\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # Use the same name for the time column: date (no timezone)\n",
    "    out[\"date\"] = pd.to_datetime(out[\"time\"], utc=True).dt.tz_localize(None)\n",
    "    out = out.drop(columns=[\"time\"])\n",
    "\n",
    "    return out[[\"lake_id\", \"lake_name\", \"date\", var_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745dbef2",
   "metadata": {},
   "source": [
    "Now, we run through all the variables and then get a merged table according to `lake_id` and `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e5fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] open tmin from /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/2024_north_america_daily/daymet_v4_daily_na_tmin_2024.nc\n",
      "[INFO] compute tmin daily mean for lake GL-1 (Erie) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 30. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compute tmin daily mean for lake GL-2 (Huron) ...\n",
      "[INFO] compute tmin daily mean for lake GL-3 (Michigan) ...\n",
      "[INFO] compute tmin daily mean for lake GL-4 (Ontario) ...\n",
      "[INFO] compute tmin daily mean for lake GL-5 (Superior) ...\n",
      "[INFO] open tmax from /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/2024_north_america_daily/daymet_v4_daily_na_tmax_2024.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 30. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compute tmax daily mean for lake GL-1 (Erie) ...\n",
      "[INFO] compute tmax daily mean for lake GL-2 (Huron) ...\n",
      "[INFO] compute tmax daily mean for lake GL-3 (Michigan) ...\n",
      "[INFO] compute tmax daily mean for lake GL-4 (Ontario) ...\n",
      "[INFO] compute tmax daily mean for lake GL-5 (Superior) ...\n",
      "[INFO] open prcp from /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/2024_north_america_daily/daymet_v4_daily_na_prcp_2024.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 30. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n",
      "/tmp/ipykernel_3455028/1403004848.py:20: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compute prcp daily mean for lake GL-1 (Erie) ...\n",
      "[INFO] compute prcp daily mean for lake GL-2 (Huron) ...\n",
      "[INFO] compute prcp daily mean for lake GL-3 (Michigan) ...\n",
      "[INFO] compute prcp daily mean for lake GL-4 (Ontario) ...\n"
     ]
    }
   ],
   "source": [
    "# Get a `df` for each variable, then merge them step by step\n",
    "dfs = []\n",
    "for var_name in VAR_FILES.keys():\n",
    "    df_var = compute_lake_daily_for_var(var_name)\n",
    "    dfs.append(df_var)\n",
    "\n",
    "# Use the first variable as the base, then merge with other variables step by step\n",
    "from functools import reduce\n",
    "\n",
    "df_daymet = reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left,\n",
    "        right,\n",
    "        on=[\"lake_id\", \"lake_name\", \"date\"],\n",
    "        how=\"outer\",\n",
    "    ),\n",
    "    dfs,\n",
    ")\n",
    "\n",
    "# Sort it for better readability\n",
    "df_daymet = df_daymet.sort_values([\"lake_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(df_daymet.head())\n",
    "print(df_daymet.describe(include=\"all\"))\n",
    "\n",
    "# Save as parquet\n",
    "OUT_DAYMET_DAILY.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_daymet.to_parquet(OUT_DAYMET_DAILY, index=False)\n",
    "print(f\"[OK] saved daily lake-mean Daymet → {OUT_DAYMET_DAILY}, rows={len(df_daymet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bd7b3",
   "metadata": {},
   "source": [
    "In this following cell of the notebook, we:\n",
    "- aggregate the daily Daymet data to weekly data;\n",
    "- compute the 28-day rolling mean of the daily data; \n",
    "- compute the monthly Daymet data; and\n",
    "- merge it with the ILW cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebd311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved weekly Daymet (7D) → /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/daymet_glakes_weekly_7D.parquet\n",
      "[OK] saved weekly Daymet (W-MON) → /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/daymet_glakes_weekly_W-MON.parquet\n",
      "[OK] saved 28D rolling mean Daymet → /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/daymet_glakes_roll28.parquet\n",
      "[OK] saved monthly Daymet → /dkucc/home/zy166/HAB-forecasting/datasets/Daymet/daymet_glakes_monthly.parquet\n",
      "[INFO] ILW daily filter qc_is_valid==1: 1805 -> 647 rows\n",
      "[OK] saved ILW+Daymet daily → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_daily_with_daymet.parquet, rows=647\n",
      "[INFO] ILW monthly filter qc_is_valid==1: 60 -> 52 rows\n",
      "[OK] saved ILW+Daymet monthly → /dkucc/home/zy166/HAB-forecasting/datasets/processed/qc/greatlakes_monthly_with_daymet.parquet, rows=52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "VARS  = [\"tmin\", \"tmax\", \"prcp\", \"srad\", \"vp\", \"dayl\"]\n",
    "\n",
    "ROOT     = Path(\"/dkucc/home/zy166/HAB-forecasting\")\n",
    "P_DAYMET_DAILY = ROOT / \"datasets/Daymet/daymet_glakes_daily.parquet\"\n",
    "\n",
    "# 1) Read the daily Daymet table\n",
    "daymet_daily = pd.read_parquet(P_DAYMET_DAILY)\n",
    "daymet_daily[\"date\"] = pd.to_datetime(daymet_daily[\"date\"])\n",
    "\n",
    "# 2) Weekly aggregation (7D or natural week W)\n",
    "# Option A: 7D rolling equal-distance window\n",
    "daymet_weekly = (\n",
    "    daymet_daily\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"lake_id\")[VARS]\n",
    "    .resample(\"7D\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "out_weekly_pq = P_DAYMET_DAILY.with_name(\"daymet_glakes_weekly_7D.parquet\")\n",
    "daymet_weekly.to_parquet(out_weekly_pq, index=False)\n",
    "print(f\"[OK] saved weekly Daymet (7D) → {out_weekly_pq}\")\n",
    "\n",
    "# Option B: Natural week (week start on Monday)\n",
    "daymet_weekly_w = (\n",
    "    daymet_daily\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"lake_id\")[VARS]\n",
    "    .resample(\"W-MON\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "out_weekly_w_pq = P_DAYMET_DAILY.with_name(\"daymet_glakes_weekly_W-MON.parquet\")\n",
    "daymet_weekly_w.to_parquet(out_weekly_w_pq, index=False)\n",
    "print(f\"[OK] saved weekly Daymet (W-MON) → {out_weekly_w_pq}\")\n",
    "\n",
    "# 3) 28-day moving average\n",
    "daymet_daily = daymet_daily.sort_values([\"lake_id\", \"date\"])\n",
    "\n",
    "roll28 = (\n",
    "    daymet_daily\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"lake_id\")[VARS]\n",
    "    .rolling(\"28D\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "# After reset_index, there are 'lake_id','date',VARS\n",
    "roll28 = roll28.rename(columns={\"date\": \"date\"})\n",
    "out_roll28_pq = P_DAYMET_DAILY.with_name(\"daymet_glakes_roll28.parquet\")\n",
    "roll28.to_parquet(out_roll28_pq, index=False)\n",
    "print(f\"[OK] saved 28D rolling mean Daymet → {out_roll28_pq}\")\n",
    "\n",
    "# 4) Monthly Daymet (align with ILW monthly data)\n",
    "daymet_monthly = (\n",
    "    daymet_daily\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"lake_id\")[VARS]\n",
    "    .resample(\"MS\")    # Month Start, align with ILW QC's \"start-of-month\"\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "out_monthly_pq = P_DAYMET_DAILY.with_name(\"daymet_glakes_monthly.parquet\")\n",
    "daymet_monthly.to_parquet(out_monthly_pq, index=False)\n",
    "print(f\"[OK] saved monthly Daymet → {out_monthly_pq}\")\n",
    "\n",
    "# 5) Merge with ILW cleaned data\n",
    "\n",
    "ROOT     = Path(\"/dkucc/home/zy166/HAB-forecasting\")\n",
    "QC_DIR   = ROOT / \"datasets/processed/qc\"\n",
    "P_ILW_D  = QC_DIR / \"greatlakes_daily_clean.parquet\"\n",
    "P_ILW_M  = QC_DIR / \"greatlakes_monthly_clean.parquet\"\n",
    "\n",
    "# Daily merge\n",
    "ilw_d = pd.read_parquet(P_ILW_D)\n",
    "ilw_d[\"date\"] = pd.to_datetime(ilw_d[\"date\"])\n",
    "\n",
    "# Only keep the observations with qc_is_valid == 1 (adjustable)\n",
    "if \"qc_is_valid\" in ilw_d.columns:\n",
    "    before = len(ilw_d)\n",
    "    ilw_d = ilw_d[ilw_d[\"qc_is_valid\"] == 1].copy()\n",
    "    print(f\"[INFO] ILW daily filter qc_is_valid==1: {before} -> {len(ilw_d)} rows\")\n",
    "\n",
    "# Merge with Daymet daily data\n",
    "merged_daily = (\n",
    "    ilw_d\n",
    "    .merge(\n",
    "        daymet_daily[[\"lake_id\", \"date\"] + VARS],\n",
    "        on=[\"lake_id\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "out_merged_daily = QC_DIR / \"greatlakes_daily_with_daymet.parquet\"\n",
    "merged_daily.to_parquet(out_merged_daily, index=False)\n",
    "print(f\"[OK] saved ILW+Daymet daily → {out_merged_daily}, rows={len(merged_daily)}\")\n",
    "\n",
    "# Monthly merge\n",
    "ilw_m = pd.read_parquet(P_ILW_M)\n",
    "# ILW QC script already ensures 'date' is the first day of the month, but here we double-check\n",
    "ilw_m[\"date\"] = pd.to_datetime(ilw_m[\"date\"])\n",
    "\n",
    "if \"qc_is_valid\" in ilw_m.columns:\n",
    "    before = len(ilw_m)\n",
    "    ilw_m = ilw_m[ilw_m[\"qc_is_valid\"] == 1].copy()\n",
    "    print(f\"[INFO] ILW monthly filter qc_is_valid==1: {before} -> {len(ilw_m)} rows\")\n",
    "\n",
    "merged_monthly = (\n",
    "    ilw_m\n",
    "    .merge(\n",
    "        daymet_monthly[[\"lake_id\", \"date\"] + VARS],\n",
    "        on=[\"lake_id\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "out_merged_monthly = QC_DIR / \"greatlakes_monthly_with_daymet.parquet\"\n",
    "merged_monthly.to_parquet(out_merged_monthly, index=False)\n",
    "print(f\"[OK] saved ILW+Daymet monthly → {out_merged_monthly}, rows={len(merged_monthly)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
