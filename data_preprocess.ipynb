{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73043a68",
   "metadata": {},
   "source": [
    "# Forecasting Freshwater Algal Bloom Levels Using Multisource Climate and Water-Quality Data\n",
    "\n",
    "*This is the course project of **STATS 402: Interdisciplinary Data Analysis**.*\n",
    "\n",
    "**Name:** Ziyue Yin\n",
    "\n",
    "**NetID:** zy166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac208a0",
   "metadata": {},
   "source": [
    "## Dataset: HydroLAKES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ef2d1",
   "metadata": {},
   "source": [
    "Lake polygons (including all attributes) in shapefileformat: https://www.hydrosheds.org/products/hydrolakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9d58b",
   "metadata": {},
   "source": [
    "## Dataset: NASA OceanColor Inland Waters (ILW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eaa6",
   "metadata": {},
   "source": [
    "S3Merged-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/Merged-S3-ILW/.\n",
    "\n",
    "S3B-ILW data: https://oceandata.sci.gsfc.nasa.gov/directdataaccess/Level-3%20Mapped/S3B-ILW/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419fb8",
   "metadata": {},
   "source": [
    "After downloading the datasets, the structure should be shown as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    " ├── ILW/\n",
    " │    ├── S3B/2024/CONUS_MO/\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3B_OLCI_EFRNT.20240201_20240229.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    ├── Merged/2024/CONUS_DAY/\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ├── S3M_OLCI_EFRNT.20240102.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\n",
    " │    │      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70cade",
   "metadata": {},
   "source": [
    "### Data Structure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84bc5",
   "metadata": {},
   "source": [
    "First of all, let's glance at the monthly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3545ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/S3B_OLCI_EFRNT.20240101_20240131.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b94fb",
   "metadata": {},
   "source": [
    "And also, the daily dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a063b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'y': 15138, 'x': 26328, 'rgb': 3, 'eightbitcolor': 256})\n",
      "['rhos_400', 'rhos_412', 'rhos_443', 'rhos_490', 'rhos_510', 'rhos_560', 'rhos_620', 'rhos_665', 'rhos_674', 'rhos_681', 'rhos_709', 'rhos_754', 'rhos_865', 'rhos_884', 'CI_cyano', 'palette']\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "p = \"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/S3M_OLCI_EFRNT.20240101.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\"\n",
    "\n",
    "ds = xr.open_dataset(p, engine=\"netcdf4\", chunks=\"auto\")\n",
    "print(ds.dims)\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b349",
   "metadata": {},
   "source": [
    "### Target Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbba54",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fe2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb166a00",
   "metadata": {},
   "source": [
    "Time Extraction: coords -> attrs -> file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d9caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_time_label(nc_path, ds, product=\"monthly\"):\n",
    "    \"\"\"\n",
    "    Return a pandas.Timestamp, try to infer from ds or filename.\n",
    "    product: 'monthly' or 'daily'\n",
    "    \"\"\"\n",
    "    # 1) Directly have time coordinate/variable\n",
    "    for k in (\"time\",):\n",
    "        if k in ds.coords or k in ds.variables:\n",
    "            try:\n",
    "                return pd.to_datetime(ds[k].values).to_pydatetime()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 2) Global attributes (common in L3M data)\n",
    "    start = ds.attrs.get(\"time_coverage_start\") or ds.attrs.get(\"start_time\")\n",
    "    end   = ds.attrs.get(\"time_coverage_end\")   or ds.attrs.get(\"end_time\")\n",
    "    if start and end:\n",
    "        try:\n",
    "            ts = pd.to_datetime(start)\n",
    "            te = pd.to_datetime(end)\n",
    "            if product == \"monthly\":\n",
    "                return ts + (te - ts) / 2\n",
    "            else:\n",
    "                return ts\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Analyze the filename\n",
    "    fn = nc_path.split(\"/\")[-1]\n",
    "    if product == \"monthly\":\n",
    "        # ...YYYYMMDD_YYYYMMDD.L3m.MO...\n",
    "        m = re.search(r\"\\.(\\d{8})_(\\d{8})\\.L3m\\.MO\\.\", fn)\n",
    "        if m:\n",
    "            b, e = m.group(1), m.group(2)\n",
    "            ts = pd.to_datetime(b, format=\"%Y%m%d\")\n",
    "            te = pd.to_datetime(e, format=\"%Y%m%d\")\n",
    "            return ts + (te - ts) / 2\n",
    "    else:\n",
    "        # ...YYYYMMDD.L3m.DAY...\n",
    "        m = re.search(r\"\\.(\\d{8})\\.L3m\\.DAY\\.\", fn)\n",
    "        if m:\n",
    "            return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "    raise ValueError(\"Cannot infer time from dataset or filename: \" + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e9e64",
   "metadata": {},
   "source": [
    "Quality Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ci(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Filter out values out of physical range and remove near-zero values.\n",
    "    \"\"\"\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin):\n",
    "        da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax):\n",
    "        da = da.where(da <= vmax)\n",
    "\n",
    "    # 去掉接近下界的小值（阈值可按需要调整）\n",
    "    thr = max(vmin, 5e-5) if np.isfinite(vmin) else 5e-5\n",
    "    da = da.where(da > thr)\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bbce3",
   "metadata": {},
   "source": [
    "For a single .nc file, get all the lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5d1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lakes_from_nc(nc_path: str,\n",
    "                          lakes_gdf: gpd.GeoDataFrame,\n",
    "                          lake_id_col: str,\n",
    "                          product: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    nc_path: A single NetCDF file (S3B monthly or S3M daily)\n",
    "    lakes_gdf: A GeoDataFrame containing `lake_id` and `geometry` (EPSG:4326)\n",
    "    product: 'monthly' | 'daily'\n",
    "    Returns: One row per lake (timestamp of the file)\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(nc_path, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    t  = infer_time_label(nc_path, ds, product=product)\n",
    "\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = set_spatial_dims_safe(da)\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    rows = []\n",
    "    for _, row in lakes_gdf.iterrows():\n",
    "        lid  = row[lake_id_col]\n",
    "        geom = [row.geometry]  # rioxarray.clip needs a list\n",
    "\n",
    "        try:\n",
    "            clipped = da.rio.clip(geom, lakes_gdf.crs, drop=True)\n",
    "            valid   = clipped.where(np.isfinite(clipped))\n",
    "            n_valid = int(valid.count().compute().values)\n",
    "            if n_valid == 0:\n",
    "                mean_val = np.nan\n",
    "                p90      = np.nan\n",
    "            else:\n",
    "                mean_val = float(valid.mean().compute().values)\n",
    "                p90      = float(valid.quantile(0.9).compute().values)\n",
    "        except Exception:\n",
    "            mean_val, p90, n_valid = np.nan, np.nan, 0\n",
    "\n",
    "        rows.append({\n",
    "            \"lake_id\": lid,\n",
    "            \"time\":   pd.to_datetime(t),\n",
    "            \"product\": product,\n",
    "            \"CI_mean\": mean_val,\n",
    "            \"CI_p90\":  p90,\n",
    "            \"n_valid\": n_valid,\n",
    "            \"src\":     Path(nc_path).name,\n",
    "        })\n",
    "\n",
    "    ds.close()\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d60dc4",
   "metadata": {},
   "source": [
    "Process monthly data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84adfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monthly(monthly_dir: str,\n",
    "                lakes_fp: str,\n",
    "                lake_id_col: str,\n",
    "                out_parquet: str):\n",
    "    \"\"\"\n",
    "    monthly_dir: Directory containing files like S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS...nc\n",
    "    lakes_fp:    Lake boundaries (gpkg/shp, must be EPSG:4326)\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(monthly_dir).glob(\"S3B_OLCI_EFRNT.*.L3m.MO.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"monthly\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No monthly files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[monthly] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a17ebf",
   "metadata": {},
   "source": [
    "Process daily data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b24609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily(daily_dir: str,\n",
    "              lakes_fp: str,\n",
    "              lake_id_col: str,\n",
    "              out_parquet: str):\n",
    "    \"\"\"\n",
    "    daily_dir: Directory containing files like S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS...nc\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(lakes_fp)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"The lake file is missing CRS, please ensure it is EPSG:4326\")\n",
    "    gdf = gdf.to_crs(4326)[[lake_id_col, \"geometry\"]].dropna()\n",
    "\n",
    "    out_rows = []\n",
    "    for fp in sorted(Path(daily_dir).glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.*.nc\")):\n",
    "        df_one = extract_lakes_from_nc(str(fp), gdf, lake_id_col, product=\"daily\")\n",
    "        out_rows.append(df_one)\n",
    "\n",
    "    if not out_rows:\n",
    "        print(\"No daily files found.\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.concat(out_rows, ignore_index=True)\n",
    "    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_all.to_parquet(out_parquet, index=False)\n",
    "    print(f\"[daily] saved → {out_parquet}  ({len(df_all)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01691f18",
   "metadata": {},
   "source": [
    "Spatial Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_spatial_dims_safe(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Try to set the spatial dimensions and CRS for L3m grids.\n",
    "    First use `x/y`; if failed, try `lon/lat`; if still failed, degrade to the last two dimensions as `x/y`.\n",
    "    \"\"\"\n",
    "    if \"x\" in da.dims and \"y\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    if \"lon\" in da.dims and \"lat\" in da.dims:\n",
    "        out = da.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    if len(da.dims) >= 2:\n",
    "        dims = list(da.dims)\n",
    "        ydim, xdim = dims[-2], dims[-1]\n",
    "        out = da.rename({xdim: \"x\", ydim: \"y\"})\n",
    "        out = out.rio.write_crs(4326)\n",
    "        out = out.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "        return out\n",
    "\n",
    "    raise ValueError(\"Cannot determine spatial dims for CI_cyano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4665a61",
   "metadata": {},
   "source": [
    "#### Scale 1: In general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f52fe",
   "metadata": {},
   "source": [
    "##### Monthly\n",
    "\n",
    "Here, we use the **S3B Monthly** data. One month per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f98a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>CI_mean</th>\n",
       "      <th>CI_p90</th>\n",
       "      <th>n_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-16 16:53:28.500000+00:00</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>2030281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 17:12:11.500000+00:00</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>2204191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-16 16:44:09+00:00</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>1810493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-16 04:52:02+00:00</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>2151149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 17:01:29+00:00</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2586974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time   CI_mean    CI_p90  n_valid\n",
       "0 2024-01-16 16:53:28.500000+00:00  0.000591  0.000472  2030281\n",
       "1 2024-02-15 17:12:11.500000+00:00  0.000539  0.000351  2204191\n",
       "2        2024-03-16 16:44:09+00:00  0.000492  0.000166  1810493\n",
       "3        2024-04-16 04:52:02+00:00  0.000496  0.000540  2151149\n",
       "4        2024-05-16 17:01:29+00:00  0.000500  0.000836  2586974"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, numpy as np, pandas as pd, xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "monthly_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO\")\n",
    "out_csv = monthly_dir/\"ci_cyano_monthly_mean.csv\"\n",
    "\n",
    "rows = []\n",
    "for fp in sorted(monthly_dir.glob(\"S3B_OLCI_EFRNT.*.L3m.MO.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "    ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "    da = ds[\"CI_cyano\"]\n",
    "    da = clean_ci(da)\n",
    "\n",
    "    vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "    vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "    if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "    if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "    m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "    p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "    t   = infer_time_label(str(fp), ds, product=\"monthly\")\n",
    "\n",
    "    rows.append({\"time\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "                 \"n_valid\": int(da.count().compute().values)})\n",
    "    ds.close()\n",
    "\n",
    "df_mo = pd.DataFrame(rows).sort_values(\"time\").reset_index(drop=True)\n",
    "df_mo.to_csv(out_csv, index=False)\n",
    "df_mo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b93b8d",
   "metadata": {},
   "source": [
    "##### Daily\n",
    "\n",
    "Here, we use the **S3M Daily** data. One day per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1970169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_dir = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY\")\n",
    "# out_csv = daily_dir/\"ci_cyano_daily_mean.csv\"\n",
    "\n",
    "# rows = []\n",
    "# for fp in sorted(daily_dir.glob(\"S3M_OLCI_EFRNT.*.L3m.DAY.ILW_CONUS.V5.all.CONUS.300m.nc\")):\n",
    "#     ds = xr.open_dataset(fp, engine=\"netcdf4\", chunks=\"auto\")\n",
    "#     da = ds[\"CI_cyano\"]\n",
    "#     da = clean_ci(da)\n",
    "\n",
    "#     vmin = float(da.attrs.get(\"valid_min\", np.nan))\n",
    "#     vmax = float(da.attrs.get(\"valid_max\", np.nan))\n",
    "#     if np.isfinite(vmin): da = da.where(da >= vmin)\n",
    "#     if np.isfinite(vmax): da = da.where(da <= vmax)\n",
    "\n",
    "#     m   = float(da.where(np.isfinite(da)).mean().compute().values)\n",
    "#     p90 = float(da.where(np.isfinite(da)).quantile(0.9).compute().values)\n",
    "#     t   = infer_time_label(str(fp), ds, product=\"daily\")\n",
    "\n",
    "#     rows.append({\"date\": pd.to_datetime(t), \"CI_mean\": m, \"CI_p90\": p90,\n",
    "#                  \"n_valid\": int(da.count().compute().values)})\n",
    "#     ds.close()\n",
    "\n",
    "# df_day = pd.DataFrame(rows).sort_values(\"date\").reset_index(drop=True)\n",
    "# df_day.to_csv(out_csv, index=False)\n",
    "# df_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc325",
   "metadata": {},
   "source": [
    "#### Scale 2: Five Great Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b2c1f",
   "metadata": {},
   "source": [
    "First of all, we get the five Great Lakes out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg features: 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "src = \"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes.gpkg\"\n",
    "gdf = gpd.read_file(src)\n",
    "\n",
    "keep_names = [\"Superior\",\"Michigan\",\"Huron\",\"Erie\",\"Ontario\"]\n",
    "gdf5 = gdf[gdf[\"Lake_name\"].str.fullmatch(\"|\".join(keep_names), case=False)].copy()\n",
    "\n",
    "# Dissolve into five polygons\n",
    "gdf5 = gdf5.dissolve(by=\"Lake_name\", as_index=False)\n",
    "\n",
    "# Clean and rename the columns\n",
    "gdf5 = gdf5.rename(columns={\"Lake_name\":\"lake_name\"})\n",
    "gdf5 = gdf5.reset_index(drop=True)\n",
    "\n",
    "# Directly overwrite/create the `lake_id` column\n",
    "gdf5[\"lake_id\"] = [f\"GL-{i+1}\" for i in range(len(gdf5))]   # GL-1..GL-5\n",
    "\n",
    "# Buffer the shoreline by 300 m, first to equidistant projection, then buffer, then back to 4326\n",
    "gdf5m = gdf5.to_crs(5070)\n",
    "gdf5m[\"geometry\"] = gdf5m.buffer(-300)\n",
    "gdf5 = gdf5m.to_crs(4326)\n",
    "\n",
    "out = Path(src).with_name(\"lakes_greatlakes_5poly.gpkg\")\n",
    "gdf5.to_file(out, driver=\"GPKG\")\n",
    "print(\"Saved:\", out, \"features:\", len(gdf5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c8b2d",
   "metadata": {},
   "source": [
    "##### Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly\n",
    "run_monthly(\n",
    "    monthly_dir=\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO\",\n",
    "    lakes_fp=\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "    lake_id_col=\"lake_id\",\n",
    "    out_parquet=\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c23b3",
   "metadata": {},
   "source": [
    "##### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily\n",
    "# run_daily(\n",
    "#     daily_dir=\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY\",\n",
    "#     lakes_fp=\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\",\n",
    "#     lake_id_col=\"lake_id\",\n",
    "#     out_parquet=\"/dkucc/home/zy166/HAB-forcasting/data/processed/lake_ci_daily.parquet\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25773dfe",
   "metadata": {},
   "source": [
    "### Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "QC & Completeness checks for NASA ILW Cyanobacteria Index (CI_cyano)\n",
    "- Regional (CONUS) daily & monthly time series\n",
    "- Lake-level (Great Lakes) daily & monthly time series\n",
    "\n",
    "Inputs (already prepared by you):\n",
    "1) /dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\n",
    "2) /dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\n",
    "3) /dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\n",
    "4) /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\n",
    "5) /dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_daily.parquet\n",
    "\n",
    "Outputs:\n",
    "- Cleaned regional daily/monthly CSVs with QC flags\n",
    "- Cleaned lake-level daily/monthly Parquet with QC flags and completeness metrics\n",
    "- Simple summary CSVs per product (row counts, missing rates, clipping, etc.)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters (tune as needed)\n",
    "# ----------------------------\n",
    "# Resolution of ILW L3m grid (meters)\n",
    "PIX_RES_M = 300.0\n",
    "\n",
    "# Minimal valid pixel ratio when judged by geometry (n_valid / expected_pixels)\n",
    "MIN_PCT_VALID_GEOM = 0.10   # drop if coverage < 10%\n",
    "\n",
    "# Minimal absolute valid pixels per lake-time\n",
    "MIN_ABS_PIX = 50            # drop very tiny coverage\n",
    "\n",
    "# Empirical coverage threshold relative to best observed coverage for that lake\n",
    "MIN_PCT_VALID_EMP = 0.10    # drop if n_valid < 10% of lake's max observed n_valid\n",
    "\n",
    "# CI cleaning thresholds\n",
    "NEAR_ZERO_THRESHOLD = 5e-5  # drop near-zero values (already applied in earlier step, kept as doc)\n",
    "CLIP_QUANTILE_LOW  = 0.001  # lower clip for outliers\n",
    "CLIP_QUANTILE_HIGH = 0.999  # upper clip for outliers\n",
    "\n",
    "# Interpolation limits for regional (optional smoothing to fill short gaps)\n",
    "INTERP_LIMIT_DAYS = 3\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "P_CONUS_DAILY   = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/Merged/2024/CONUS_DAY/ci_cyano_daily_mean.csv\")\n",
    "P_CONUS_MONTHLY = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/ILW/S3B/2024/CONUS_MO/ci_cyano_monthly_mean.csv\")\n",
    "P_LAKES_GPKG    = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/Lakes/shapes/lakes_greatlakes_5poly.gpkg\")\n",
    "P_LAKE_DAILY    = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_daily.parquet\")\n",
    "P_LAKE_MONTHLY  = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/lake_ci_monthly.parquet\")\n",
    "\n",
    "OUT_DIR = Path(\"/dkucc/home/zy166/HAB-forcasting/datasets/processed/qc\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _clip_series_q(s: pd.Series, qlow=CLIP_QUANTILE_LOW, qhigh=CLIP_QUANTILE_HIGH) -> pd.Series:\n",
    "    \"\"\"Clip a numeric series by quantiles; preserve NaNs.\"\"\"\n",
    "    if s.dropna().empty:\n",
    "        return s\n",
    "    lo = s.quantile(qlow)\n",
    "    hi = s.quantile(qhigh)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame,\n",
    "                     prefer_cols=(\"date\", \"time\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust datetime parsing:\n",
    "    - try 'date' then 'time' (or any column that contains these names)\n",
    "    - use pandas 'mixed' parser and coerce to UTC tz-aware -> drop tz\n",
    "    - drop NaT rows\n",
    "    \"\"\"\n",
    "    # find a candidate column\n",
    "    col = None\n",
    "    for cand in prefer_cols:\n",
    "        if cand in df.columns:\n",
    "            col = cand\n",
    "            break\n",
    "    if col is None:\n",
    "        # try fuzzy match by column name\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if \"date\" in lc or \"time\" in lc or \"datetime\" in lc or \"timestamp\" in lc:\n",
    "                col = c\n",
    "                break\n",
    "    if col is None:\n",
    "        raise ValueError(\"No recognizable datetime column (expect 'date'/'time').\")\n",
    "\n",
    "    # mixed-format tolerant parsing; force UTC then remove tz\n",
    "    # errors='coerce' will set unparsable entries to NaT (we drop them)\n",
    "    dt = pd.to_datetime(df[col], utc=True, format=\"mixed\", errors=\"coerce\")\n",
    "    dt = dt.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = dt\n",
    "    # drop rows that failed to parse\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "    if len(df) < before:\n",
    "        print(f\"[QC] Dropped {before - len(df)} rows with unparseable datetime in column '{col}'\")\n",
    "    return df\n",
    "\n",
    "def _summarize_basic(df: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
    "    \"\"\"Basic counts and missingness summary for quick logging.\"\"\"\n",
    "    out = {\n",
    "        \"tag\": tag,\n",
    "        \"rows\": len(df),\n",
    "        \"n_missing_CI_mean\": int(df[\"CI_mean\"].isna().sum()) if \"CI_mean\" in df.columns else None,\n",
    "        \"n_missing_CI_p90\":  int(df[\"CI_p90\"].isna().sum()) if \"CI_p90\" in df.columns else None,\n",
    "        \"n_missing_n_valid\": int(df[\"n_valid\"].isna().sum()) if \"n_valid\" in df.columns else None,\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "# ----------------------------\n",
    "# QC for regional (CONUS) time series\n",
    "# ----------------------------\n",
    "def qc_conus_timeseries(csv_path: Path, freq: str, out_prefix: str) -> None:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    expected = {\"CI_mean\", \"CI_p90\", \"n_valid\"}\n",
    "    missing = expected - set(df.columns)\n",
    "    if missing:\n",
    "        print(f\"[WARN] {csv_path.name} is missing columns: {missing}\")\n",
    "\n",
    "    # robust parse + drop NaT\n",
    "    df = _ensure_datetime(df, prefer_cols=(\"date\", \"time\"))\n",
    "\n",
    "    # normalize timestamps to start-of-period\n",
    "    if freq.upper() == \"D\":\n",
    "        df[\"date\"] = df[\"date\"].dt.floor(\"D\")\n",
    "    elif freq.upper() == \"MS\":\n",
    "        # month start\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp(\"MS\")\n",
    "    else:\n",
    "        # fallback: floor to given freq\n",
    "        df[\"date\"] = df[\"date\"].dt.to_period(freq).dt.start_time\n",
    "\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # build regular index over the observed span\n",
    "    idx = pd.date_range(df[\"date\"].min(), df[\"date\"].max(), freq=freq)\n",
    "    df = df.set_index(\"date\").reindex(idx).rename_axis(\"date\").reset_index()\n",
    "\n",
    "    # short-gap interpolation on CI columns\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].interpolate(limit=INTERP_LIMIT_DAYS if freq.upper()==\"D\" else 1)\n",
    "\n",
    "    # robust quantile clipping\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _clip_series_q(df[c])\n",
    "\n",
    "    if \"n_valid\" in df.columns:\n",
    "        nv_thr = df[\"n_valid\"].quantile(0.05) if df[\"n_valid\"].notna().any() else 0\n",
    "        df[\"qc_low_coverage\"] = (df[\"n_valid\"] < nv_thr).astype(int)\n",
    "\n",
    "    out_csv = OUT_DIR / f\"{out_prefix}_clean.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    _summarize_basic(df, tag=f\"{out_prefix}\").to_csv(OUT_DIR / f\"{out_prefix}_summary.csv\", index=False)\n",
    "    print(f\"[OK] Saved regional QC → {out_csv}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Lake geometry → expected pixel count\n",
    "# ----------------------------\n",
    "def compute_expected_pixels_per_lake(lakes_gpkg: Path,\n",
    "                                     inset_m: float = 300.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute expected pixel counts per lake from geometry area, after optional inward buffer\n",
    "    to reduce shoreline contamination. Done in equal-area CRS (EPSG:5070) then back to 4326.\n",
    "\n",
    "    Returns a DataFrame: lake_id, lake_name (if present), expected_pixels_geom, area_m2\n",
    "    \"\"\"\n",
    "    g = gpd.read_file(lakes_gpkg)\n",
    "    # Normalize lake_id and names\n",
    "    cols = list(g.columns)\n",
    "    if \"lake_id\" not in cols:\n",
    "        raise ValueError(\"Expect 'lake_id' in lakes_greatlakes_5poly.gpkg\")\n",
    "\n",
    "    name_col = None\n",
    "    for cand in [\"lake_name\", \"Lake_name\", \"name\", \"Name\"]:\n",
    "        if cand in cols:\n",
    "            name_col = cand\n",
    "            break\n",
    "\n",
    "    # Reproject to equal-area CRS for stable buffering & area\n",
    "    gm = g.to_crs(5070)\n",
    "\n",
    "    # Optional inward buffer to avoid shoreline pixels\n",
    "    if inset_m and inset_m > 0:\n",
    "        gm[\"geometry\"] = gm.buffer(-abs(inset_m))\n",
    "\n",
    "    gm[\"area_m2\"] = gm.geometry.area\n",
    "    # Expected pixel count at given resolution\n",
    "    gm[\"expected_pixels_geom\"] = (gm[\"area_m2\"] / (PIX_RES_M ** 2)).round().astype(\"Int64\")\n",
    "\n",
    "    df = gm.to_crs(4326)[[\"lake_id\", \"area_m2\", \"expected_pixels_geom\"]].copy()\n",
    "    if name_col:\n",
    "        df[name_col] = g[name_col].values\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# QC for lake-level time series\n",
    "# ----------------------------\n",
    "def qc_lake_timeseries(parquet_path: Path, lakes_gpkg: Path, out_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    QC for lake-level CI time series (daily or monthly parquet).\n",
    "    Adds:\n",
    "      - expected_pixels based on lake geometry\n",
    "      - pct_valid_geom = n_valid / expected_pixels\n",
    "      - pct_valid_emp = n_valid / max(n_valid) per lake\n",
    "      - QC flags: low coverage (geom/emp), tiny absolute pixels, and clipped values\n",
    "    Outputs cleaned Parquet + summary CSV.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = _ensure_datetime(df, col=\"time\")\n",
    "    # Standardize column names\n",
    "    if \"time\" in df.columns:\n",
    "        df = df.rename(columns={\"time\": \"date\"})\n",
    "    if \"product\" not in df.columns:\n",
    "        df[\"product\"] = \"unknown\"\n",
    "\n",
    "    # Expected pixels per lake from geometry\n",
    "    geom_df = compute_expected_pixels_per_lake(lakes_gpkg, inset_m=PIX_RES_M)  # inset = 1 pixel\n",
    "    name_col = \"lake_name\" if \"lake_name\" in geom_df.columns else (\"Lake_name\" if \"Lake_name\" in geom_df.columns else None)\n",
    "\n",
    "    # Merge expected pixels\n",
    "    merge_cols = [\"lake_id\"]\n",
    "    df = df.merge(geom_df, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "    # Empirical max n_valid per lake\n",
    "    if \"n_valid\" in df.columns:\n",
    "        emp_max = df.groupby(\"lake_id\")[\"n_valid\"].max().rename(\"empiric_n_valid_max\")\n",
    "        df = df.merge(emp_max, on=\"lake_id\", how=\"left\")\n",
    "\n",
    "        # Coverage metrics\n",
    "        df[\"pct_valid_geom\"] = df[\"n_valid\"] / df[\"expected_pixels_geom\"]\n",
    "        df[\"pct_valid_emp\"]  = df[\"n_valid\"] / df[\"empiric_n_valid_max\"].replace(0, np.nan)\n",
    "\n",
    "        # QC coverage flags\n",
    "        df[\"qc_low_cov_geom\"] = (df[\"pct_valid_geom\"] < MIN_PCT_VALID_GEOM).astype(\"Int64\")\n",
    "        df[\"qc_low_cov_emp\"]  = (df[\"pct_valid_emp\"]  < MIN_PCT_VALID_EMP ).astype(\"Int64\")\n",
    "        df[\"qc_tiny_abs_pix\"] = (df[\"n_valid\"] < MIN_ABS_PIX).astype(\"Int64\")\n",
    "    else:\n",
    "        # n_valid missing; create empty coverage columns\n",
    "        for c in [\"pct_valid_geom\", \"pct_valid_emp\", \"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # Clip CI columns lake-wise (robust to per-lake distributions)\n",
    "    for c in [\"CI_mean\", \"CI_p90\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df.groupby(\"lake_id\", group_keys=False)[c].apply(\n",
    "                lambda s: _clip_series_q(s, CLIP_QUANTILE_LOW, CLIP_QUANTILE_HIGH)\n",
    "            )\n",
    "\n",
    "    # A consolidated QC validity flag\n",
    "    # Valid if NOT any of (low cov geom, low cov emp, tiny abs pix)\n",
    "    cov_flags = [\"qc_low_cov_geom\", \"qc_low_cov_emp\", \"qc_tiny_abs_pix\"]\n",
    "    df[\"qc_is_valid\"] = 1\n",
    "    for c in cov_flags:\n",
    "        if c in df.columns:\n",
    "            df[\"qc_is_valid\"] = df[\"qc_is_valid\"] & (df[c] != 1)\n",
    "\n",
    "    # Save cleaned parquet\n",
    "    out_pq = OUT_DIR / f\"{out_prefix}_clean.parquet\"\n",
    "    df.to_parquet(out_pq, index=False)\n",
    "\n",
    "    # Build simple summary per-lake\n",
    "    summary_rows = []\n",
    "    for lid, g in df.groupby(\"lake_id\"):\n",
    "        n_rows = len(g)\n",
    "        n_valid_rows = int((g[\"qc_is_valid\"] == 1).sum())\n",
    "        frac_valid = n_valid_rows / n_rows if n_rows else 0.0\n",
    "        row = {\n",
    "            \"lake_id\": lid,\n",
    "            \"rows\": n_rows,\n",
    "            \"rows_valid\": n_valid_rows,\n",
    "            \"frac_valid\": round(frac_valid, 4),\n",
    "            \"pct_valid_geom_med\": float(np.nanmedian(g[\"pct_valid_geom\"])) if \"pct_valid_geom\" in g.columns else np.nan,\n",
    "            \"pct_valid_emp_med\":  float(np.nanmedian(g[\"pct_valid_emp\"]))  if \"pct_valid_emp\"  in g.columns else np.nan,\n",
    "        }\n",
    "        if name_col and name_col in g.columns:\n",
    "            row[\"lake_name\"] = g[name_col].iloc[0]\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"lake_id\")\n",
    "    out_summary = OUT_DIR / f\"{out_prefix}_summary.csv\"\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "\n",
    "    print(f\"[OK] Saved lake-level QC → {out_pq}\")\n",
    "    print(f\"[OK] Summary → {out_summary}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Regional time series QC\n",
    "    if P_CONUS_DAILY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_DAILY,   freq=\"D\",  out_prefix=\"conus_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_DAILY}\")\n",
    "\n",
    "    if P_CONUS_MONTHLY.exists():\n",
    "        qc_conus_timeseries(P_CONUS_MONTHLY, freq=\"MS\", out_prefix=\"conus_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_CONUS_MONTHLY}\")\n",
    "\n",
    "    # 2) Lake-level QC (Great Lakes)\n",
    "    if P_LAKE_DAILY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_DAILY,  P_LAKES_GPKG, out_prefix=\"greatlakes_daily\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_DAILY}\")\n",
    "\n",
    "    if P_LAKE_MONTHLY.exists():\n",
    "        qc_lake_timeseries(P_LAKE_MONTHLY, P_LAKES_GPKG, out_prefix=\"greatlakes_monthly\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Missing: {P_LAKE_MONTHLY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead180bf",
   "metadata": {},
   "source": [
    "### Standard Preprocessed Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a1a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c80f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e31020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
